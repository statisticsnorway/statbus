--
-- Test: Import jobs for BRREG selection data (~29K rows)
--
-- This test uses multiple transactions to allow worker.process_tasks() to
-- commit between tasks, avoiding O(n^2) performance issues.
--
-- CRITICAL: Upload order determines processing priority!
-- - LU (hovedenhet) must be uploaded before ES (underenhet)
-- - ES depends on LU external_idents which are created during LU processing
--
-- Performance benchmark data is written to:
--   test/expected/performance/401_import_benchmark.perf
--
-- ============================================================================
-- PHASE 0: SETUP
-- ============================================================================
-- Note: No reset needed - 4xx tests run in isolated databases from template
\echo 'Pausing background worker for test duration'
Pausing background worker for test duration
SELECT worker.pause('1 hour'::interval);
 pause 
-------
 
(1 row)

-- ============================================================================
-- PHASE 1: SETUP (in transaction so setup.sql helpers work)
-- ============================================================================
BEGIN;
\i test/setup.sql
\echo -- test/setup.sql output suppressed for cleaner test output
-- test/setup.sql output suppressed for cleaner test output
\set ECHO none
\echo -- test/setup.sql done, test output follows
-- test/setup.sql done, test output follows
\echo "Setting up Statbus (Norway) and BRREG import definitions (2024)"
"Setting up Statbus (Norway) and BRREG import definitions (2024)"
\i samples/norway/getting-started.sql
\i samples/norway/settings.sql
INSERT INTO settings(activity_category_standard_id,country_id)
SELECT (SELECT id FROM activity_category_standard WHERE code = 'nace_v2.1')
     , (SELECT id FROM public.country WHERE iso_2 = 'NO')
ON CONFLICT (only_one_setting)
DO UPDATE SET
   activity_category_standard_id = EXCLUDED.activity_category_standard_id,
   country_id = EXCLUDED.country_id
   WHERE settings.only_one_setting = EXCLUDED.only_one_setting;
;
\i samples/norway/activity_category/activity_category_norway.sql
\copy public.activity_category_available_custom FROM 'samples/norway/activity_category/activity_category_norway.csv' WITH (FORMAT csv, DELIMITER ',', QUOTE '"', HEADER true);
\i samples/norway/regions/norway-regions-2024.sql
\copy public.region_upload(path, name) FROM 'samples/norway/regions/norway-regions-2024.csv' WITH (FORMAT csv, DELIMITER ',', QUOTE '"', HEADER true);
\i samples/norway/sector/sector_norway.sql
\copy public.sector_custom_only FROM 'samples/norway/sector/sector_norway.csv' WITH (FORMAT csv, DELIMITER ',', QUOTE '"', HEADER true);
\i samples/norway/legal_form/legal_form_norway.sql
\copy public.legal_form_custom_only FROM 'samples/norway/legal_form/legal_form_norway.csv' WITH (FORMAT csv, DELIMITER ',', QUOTE '"', HEADER true);
\i samples/norway/data_source/data_source_norway.sql
\copy public.data_source_custom (code, name) FROM 'samples/norway/data_source/data_source_norway.csv' WITH (FORMAT csv, DELIMITER ',', QUOTE '"', HEADER true);
\i samples/norway/brreg/create-import-definition-hovedenhet-2024.sql
-- Create import definition for BRREG Hovedenhet (legal_unit) using 2024 columns
-- This version uses the new import system with steps and helper functions.
DO $$
DECLARE
    def_id INT;
    -- Define the steps needed for a legal unit import with a time context
    lu_steps TEXT[] := ARRAY[
        'external_idents', 'data_source', 'enterprise_link_for_legal_unit', 'valid_time', 'status', 'legal_unit',
        'physical_location', 'postal_location', 'primary_activity', 'secondary_activity',
        'contact', 'statistical_variables', 'tags', 'edit_info', 'metadata'
    ];
    map_rec RECORD;
    v_source_col_id INT;
    v_data_col_id INT;
BEGIN
    -- 1. Create the definition record (initially invalid)
    INSERT INTO public.import_definition (slug, name, note, strategy, mode, valid_time_from, valid, data_source_id)
    VALUES ('brreg_hovedenhet_2024', 'Import of BRREG Hovedenhet using 2024 columns', 'Easy upload of the CSV file found at brreg.', 'insert_or_update', 'legal_unit', 'job_provided', false, (SELECT id FROM public.data_source WHERE code = 'brreg'))
    RETURNING id INTO def_id;

    -- 2. Link the required steps to the definition
    PERFORM import.link_steps_to_definition(def_id, lu_steps);

    -- 3. Data columns are defined per step and generated by lifecycle callbacks.
    --    The call to admin.create_data_columns_for_definition(def_id) is removed.

    -- 4. Define source columns and mappings declaratively
    -- Create a temporary table to hold the mapping list
    CREATE TEMP TABLE temp_mapping_list (
        priority INT,
        source_name TEXT,
        target_name TEXT
    ) ON COMMIT DROP;

    -- Populate the temporary table
    INSERT INTO temp_mapping_list (priority, source_name, target_name)
    SELECT ROW_NUMBER() OVER () as priority, source_name, target_name
    FROM (VALUES
            ('organisasjonsnummer', 'tax_ident'),
            ('navn', 'name'),
            ('organisasjonsform.kode', 'legal_form_code'),
            ('organisasjonsform.beskrivelse', NULL), -- Ignored
            ('naeringskode1.kode', 'primary_activity_category_code'),
            ('naeringskode1.beskrivelse', NULL), -- Ignored
            ('naeringskode2.kode', 'secondary_activity_category_code'),
            ('naeringskode2.beskrivelse', NULL), -- Ignored
            ('naeringskode3.kode', NULL), -- Ignored
            ('naeringskode3.beskrivelse', NULL), -- Ignored
            ('hjelpeenhetskode.kode', NULL), -- Ignored
            ('hjelpeenhetskode.beskrivelse', NULL), -- Ignored
            ('harRegistrertAntallAnsatte', NULL), -- Ignored
            ('antallAnsatte', 'employees'),
            ('hjemmeside', 'web_address'),
            ('postadresse.adresse', 'postal_address_part1'),
            ('postadresse.poststed', 'postal_postplace'),
            ('postadresse.postnummer', 'postal_postcode'),
            ('postadresse.kommune', NULL), -- Ignored
            ('postadresse.kommunenummer', 'postal_region_code'),
            ('postadresse.land', NULL), -- Ignored
            ('postadresse.landkode', 'postal_country_iso_2'),
            ('forretningsadresse.adresse', 'physical_address_part1'),
            ('forretningsadresse.poststed', 'physical_postplace'),
            ('forretningsadresse.postnummer', 'physical_postcode'),
            ('forretningsadresse.kommune', NULL), -- Ignored
            ('forretningsadresse.kommunenummer', 'physical_region_code'),
            ('forretningsadresse.land', NULL), -- Ignored
            ('forretningsadresse.landkode', 'physical_country_iso_2'),
            ('institusjonellSektorkode.kode', 'sector_code'),
            ('institusjonellSektorkode.beskrivelse', NULL), -- Ignored
            ('sisteInnsendteAarsregnskap', NULL), -- Ignored
            ('registreringsdatoenhetsregisteret', NULL), -- Ignored
            ('stiftelsesdato', 'birth_date'),
            ('registrertIMvaRegisteret', NULL), -- Ignored
            ('frivilligMvaRegistrertBeskrivelser', NULL), -- Ignored
            ('registrertIFrivillighetsregisteret', NULL), -- Ignored
            ('registrertIForetaksregisteret', NULL), -- Ignored
            ('registrertIStiftelsesregisteret', NULL), -- Ignored
            ('konkurs', NULL), -- Ignored
            ('konkursdato', NULL), -- Ignored
            ('underAvvikling', NULL), -- Ignored
            ('underAvviklingDato', NULL), -- Ignored
            ('underTvangsavviklingEllerTvangsopplosning', NULL), -- Ignored
            ('tvangsopplostPgaManglendeDagligLederDato', NULL), -- Ignored
            ('tvangsopplostPgaManglendeRevisorDato', NULL), -- Ignored
            ('tvangsopplostPgaManglendeRegnskapDato', NULL), -- Ignored
            ('tvangsopplostPgaMangelfulltStyreDato', NULL), -- Ignored
            ('tvangsavvikletPgaManglendeSlettingDato', NULL), -- Ignored
            ('overordnetEnhet', NULL), -- Ignored (Handled by link step if needed)
            ('maalform', NULL), -- Ignored
            ('vedtektsdato', NULL), -- Ignored
            ('vedtektsfestetFormaal', NULL), -- Ignored
            ('aktivitet', NULL) -- Ignored
        ) AS v(source_name, target_name);

    -- Loop through the temporary mapping list table
    FOR map_rec IN SELECT * FROM temp_mapping_list ORDER BY priority LOOP
        -- Insert the source column
        INSERT INTO public.import_source_column (definition_id, column_name, priority)
        VALUES (def_id, map_rec.source_name, map_rec.priority)
        RETURNING id INTO v_source_col_id;

        -- If a target is specified, create the mapping
        IF map_rec.target_name IS NOT NULL THEN
            -- Find the corresponding data column ID by joining through the steps linked to this definition
            SELECT dc.id INTO v_data_col_id
            FROM public.import_definition_step ds
            JOIN public.import_data_column dc ON ds.step_id = dc.step_id
            WHERE ds.definition_id = def_id
              AND dc.column_name = map_rec.target_name || '_raw'
              AND dc.purpose = 'source_input';

            IF v_data_col_id IS NOT NULL THEN
                -- Insert the mapping
                INSERT INTO public.import_mapping (definition_id, source_column_id, target_data_column_id, target_data_column_purpose, is_ignored)
                VALUES (def_id, v_source_col_id, v_data_col_id, 'source_input'::public.import_data_column_purpose, FALSE)
                ON CONFLICT (definition_id, source_column_id, target_data_column_id) DO NOTHING;
            ELSE
                -- If target_name was specified but no data_col_id found, or if target_name was NULL initially.
                INSERT INTO public.import_mapping (definition_id, source_column_id, is_ignored, target_data_column_id, target_data_column_purpose)
                VALUES (def_id, v_source_col_id, TRUE, NULL, NULL)
                ON CONFLICT (definition_id, source_column_id, target_data_column_id) WHERE target_data_column_id IS NULL
                DO NOTHING;
            END IF;
        ELSE -- map_rec.target_name IS NULL
            INSERT INTO public.import_mapping (definition_id, source_column_id, is_ignored, target_data_column_id, target_data_column_purpose)
            VALUES (def_id, v_source_col_id, TRUE, NULL, NULL)
            ON CONFLICT (definition_id, source_column_id, target_data_column_id) WHERE target_data_column_id IS NULL
            DO NOTHING;
        END IF;
    END LOOP;

    -- 4c. Add mappings for default values (valid_from, valid_to) for job_provided definitions
    -- These source columns don't exist, so source_column_id is NULL
    DECLARE
        v_valid_time_step_id INT;
    BEGIN
        SELECT id INTO v_valid_time_step_id FROM public.import_step WHERE code = 'valid_time';

        INSERT INTO public.import_mapping (definition_id, source_expression, target_data_column_id)
        SELECT def_id, 'default'::public.import_source_expression, dc.id
        FROM public.import_data_column dc
        WHERE dc.step_id = v_valid_time_step_id
          AND dc.column_name IN ('valid_from_raw', 'valid_to_raw')
          AND dc.purpose = 'source_input'
        ON CONFLICT DO NOTHING;
    END;


    -- 5. Set the 'tax_ident' data column as uniquely identifying for the prepare step UPSERT
    DECLARE
        v_idents_step_id INT;
    BEGIN
        SELECT id INTO v_idents_step_id FROM public.import_step WHERE code = 'external_idents';
        UPDATE public.import_data_column
        SET is_uniquely_identifying = true
        WHERE step_id = v_idents_step_id -- Use step_id
          AND column_name = 'tax_ident_raw'
          AND purpose = 'source_input';
    END; -- End of the inner BEGIN/END block
    
    DROP TABLE temp_mapping_list;

END $$; -- End of the main DO block
-- Display the created definition details
SELECT d.slug,
       d.name,
       d.note,
       ds.code as data_source,
       d.valid_time_from,
       d.strategy,
       d.valid,
       d.validation_error
FROM public.import_definition d
LEFT JOIN public.data_source ds ON ds.id = d.data_source_id
WHERE d.slug = 'brreg_hovedenhet_2024';
         slug          |                     name                      |                    note                     | data_source | valid_time_from |     strategy     | valid | validation_error 
-----------------------+-----------------------------------------------+---------------------------------------------+-------------+-----------------+------------------+-------+------------------
 brreg_hovedenhet_2024 | Import of BRREG Hovedenhet using 2024 columns | Easy upload of the CSV file found at brreg. | brreg       | job_provided    | insert_or_update | t     | 
(1 row)

-- Set the definition to valid (can now be used to create jobs)
UPDATE public.import_definition
SET valid = true, validation_error = NULL
WHERE slug = 'brreg_hovedenhet_2024';
\i samples/norway/brreg/create-import-definition-underenhet-2024.sql
-- Create import definition for BRREG Underenhet (establishment for legal unit) using 2024 columns
-- This version uses the new import system with steps and helper functions.
DO $$
DECLARE
    def_id INT;
    -- Define the steps needed for an establishment linked to a legal unit import with a time context
    es_steps TEXT[] := ARRAY[
        'external_idents', 'data_source', 'link_establishment_to_legal_unit', 'valid_time', 'status', 'establishment',
        'physical_location', 'postal_location', 'primary_activity', 'secondary_activity',
        'contact', 'statistical_variables', 'tags', 'edit_info', 'metadata'
    ];
    map_rec RECORD;
    v_source_col_id INT;
    v_data_col_id INT;
BEGIN
    -- 1. Create the definition record (initially invalid)
    INSERT INTO public.import_definition (slug, name, note, strategy, mode, valid_time_from, valid, data_source_id)
    VALUES ('brreg_underenhet_2024', 'Import of BRREG Underenhet using 2024 columns', 'Easy upload of the CSV file found at brreg.', 'insert_or_update', 'establishment_formal', 'job_provided', false, (SELECT id FROM public.data_source WHERE code = 'brreg'))
    RETURNING id INTO def_id;

    -- 2. Link the required steps to the definition
    PERFORM import.link_steps_to_definition(def_id, es_steps);

    -- 3. Data columns are defined per step and generated by lifecycle callbacks.
    --    The call to admin.create_data_columns_for_definition(def_id) is removed.

    -- 4. Define source columns and mappings declaratively
    -- Create a temporary table to hold the mapping list
    CREATE TEMP TABLE temp_mapping_list (
        priority INT,
        source_name TEXT,
        target_name TEXT
    ) ON COMMIT DROP;

    -- Populate the temporary table
    INSERT INTO temp_mapping_list (priority, source_name, target_name)
    SELECT ROW_NUMBER() OVER () as priority, source_name, target_name
    FROM (VALUES
            ('organisasjonsnummer', 'tax_ident'),
            ('navn', 'name'),
            ('organisasjonsform.kode', NULL), -- Ignored
            ('organisasjonsform.beskrivelse', NULL), -- Ignored
            ('naeringskode1.kode', 'primary_activity_category_code'),
            ('naeringskode1.beskrivelse', NULL), -- Ignored
            ('naeringskode2.kode', 'secondary_activity_category_code'),
            ('naeringskode2.beskrivelse', NULL), -- Ignored
            ('naeringskode3.kode', NULL), -- Ignored
            ('naeringskode3.beskrivelse', NULL), -- Ignored
            ('hjelpeenhetskode.kode', NULL), -- Ignored
            ('hjelpeenhetskode.beskrivelse', NULL), -- Ignored
            ('harRegistrertAntallAnsatte', NULL), -- Ignored
            ('antallAnsatte', 'employees'),
            ('hjemmeside', 'web_address'),
            ('postadresse.adresse', 'postal_address_part1'),
            ('postadresse.poststed', 'postal_postplace'),
            ('postadresse.postnummer', 'postal_postcode'),
            ('postadresse.kommune', NULL), -- Ignored
            ('postadresse.kommunenummer', 'postal_region_code'),
            ('postadresse.land', NULL), -- Ignored
            ('postadresse.landkode', 'postal_country_iso_2'),
            ('beliggenhetsadresse.adresse', 'physical_address_part1'),
            ('beliggenhetsadresse.poststed', 'physical_postplace'),
            ('beliggenhetsadresse.postnummer', 'physical_postcode'),
            ('beliggenhetsadresse.kommune', NULL), -- Ignored
            ('beliggenhetsadresse.kommunenummer', 'physical_region_code'),
            ('beliggenhetsadresse.land', NULL), -- Ignored
            ('beliggenhetsadresse.landkode', 'physical_country_iso_2'),
            ('registreringsdatoIEnhetsregisteret', NULL), -- Ignored
            ('frivilligMvaRegistrertBeskrivelser', NULL), -- Ignored
            ('registrertIMvaregisteret', NULL), -- Ignored
            ('oppstartsdato', 'birth_date'),
            ('datoEierskifte', NULL), -- Ignored
            ('overordnetEnhet', 'legal_unit_tax_ident'), -- Map to the dynamic column
            ('nedleggelsesdato', 'death_date')
        ) AS v(source_name, target_name);

    -- Loop through the temporary mapping list table
    FOR map_rec IN SELECT * FROM temp_mapping_list ORDER BY priority LOOP
        -- Insert the source column
        INSERT INTO public.import_source_column (definition_id, column_name, priority)
        VALUES (def_id, map_rec.source_name, map_rec.priority)
        RETURNING id INTO v_source_col_id;

        -- If a target is specified, create the mapping
        IF map_rec.target_name IS NOT NULL THEN
            -- Find the corresponding data column ID by joining through the steps linked to this definition
            SELECT dc.id INTO v_data_col_id
            FROM public.import_definition_step ds
            JOIN public.import_data_column dc ON ds.step_id = dc.step_id
            WHERE ds.definition_id = def_id
              AND dc.column_name = map_rec.target_name || '_raw'
              AND dc.purpose = 'source_input';

            IF v_data_col_id IS NOT NULL THEN
                -- Insert the mapping
                INSERT INTO public.import_mapping (definition_id, source_column_id, target_data_column_id, target_data_column_purpose, is_ignored)
                VALUES (def_id, v_source_col_id, v_data_col_id, 'source_input'::public.import_data_column_purpose, FALSE)
                ON CONFLICT (definition_id, source_column_id, target_data_column_id) DO NOTHING;
            ELSE
                -- If target_name was specified but no data_col_id found, or if target_name was NULL initially.
                INSERT INTO public.import_mapping (definition_id, source_column_id, is_ignored, target_data_column_id, target_data_column_purpose)
                VALUES (def_id, v_source_col_id, TRUE, NULL, NULL)
                ON CONFLICT (definition_id, source_column_id, target_data_column_id) WHERE target_data_column_id IS NULL
                DO NOTHING;
            END IF;
        ELSE -- map_rec.target_name IS NULL
            INSERT INTO public.import_mapping (definition_id, source_column_id, is_ignored, target_data_column_id, target_data_column_purpose)
            VALUES (def_id, v_source_col_id, TRUE, NULL, NULL)
            ON CONFLICT (definition_id, source_column_id, target_data_column_id) WHERE target_data_column_id IS NULL
            DO NOTHING;
        END IF;
    END LOOP;

    -- 4c. Add mappings for default values (valid_from, valid_to) for job_provided definitions
    -- These source columns don't exist, so source_column_id is NULL
    DECLARE
        v_valid_time_step_id INT;
    BEGIN
        SELECT id INTO v_valid_time_step_id FROM public.import_step WHERE code = 'valid_time';

        INSERT INTO public.import_mapping (definition_id, source_expression, target_data_column_id)
        SELECT def_id, 'default'::public.import_source_expression, dc.id
        FROM public.import_data_column dc
        WHERE dc.step_id = v_valid_time_step_id
          AND dc.column_name IN ('valid_from_raw', 'valid_to_raw')
          AND dc.purpose = 'source_input'
        ON CONFLICT DO NOTHING;
    END;


    -- 5. Set the 'tax_ident' data column as uniquely identifying for the prepare step UPSERT
    DECLARE
        v_idents_step_id INT;
    BEGIN
        SELECT id INTO v_idents_step_id FROM public.import_step WHERE code = 'external_idents';
        UPDATE public.import_data_column
        SET is_uniquely_identifying = true
        WHERE step_id = v_idents_step_id -- Use step_id
          AND column_name = 'tax_ident_raw'
          AND purpose = 'source_input';
    END; -- End of the inner BEGIN/END block

    DROP TABLE temp_mapping_list;
    
END $$; -- End of the main DO block
-- Display the created definition details
SELECT d.slug,
       d.name,
       d.note,
       ds.code as data_source,
       d.valid_time_from,
       d.strategy,
       d.valid,
       d.validation_error
FROM public.import_definition d
LEFT JOIN public.data_source ds ON ds.id = d.data_source_id
WHERE d.slug = 'brreg_underenhet_2024';
         slug          |                     name                      |                    note                     | data_source | valid_time_from |     strategy     | valid | validation_error 
-----------------------+-----------------------------------------------+---------------------------------------------+-------------+-----------------+------------------+-------+------------------
 brreg_underenhet_2024 | Import of BRREG Underenhet using 2024 columns | Easy upload of the CSV file found at brreg. | brreg       | job_provided    | insert_or_update | t     | 
(1 row)

-- Set the definition to valid (can now be used to create jobs)
UPDATE public.import_definition
SET valid = true, validation_error = NULL
WHERE slug = 'brreg_underenhet_2024';
\echo "Switch to test admin user"
"Switch to test admin user"
CALL test.set_user_from_email('test.admin@statbus.org');
\echo "Verify import definitions exist"
"Verify import definitions exist"
SELECT slug, name, mode
  FROM public.import_definition
 WHERE slug IN ('brreg_hovedenhet_2024','brreg_underenhet_2024')
 ORDER BY slug;
         slug          |                     name                      |         mode         
-----------------------+-----------------------------------------------+----------------------
 brreg_hovedenhet_2024 | Import of BRREG Hovedenhet using 2024 columns | legal_unit
 brreg_underenhet_2024 | Import of BRREG Underenhet using 2024 columns | establishment_formal
(2 rows)

-- ============================================================================
-- Create import jobs - LU first, then ES (upload order = processing priority)
-- ============================================================================
\echo "Create import job for LU (hovedenhet) - uploaded FIRST for priority"
"Create import job for LU (hovedenhet) - uploaded FIRST for priority"
WITH def_he AS (
  SELECT id FROM public.import_definition WHERE slug = 'brreg_hovedenhet_2024'
)
INSERT INTO public.import_job (
  definition_id,
  slug,
  default_valid_from,
  default_valid_to,
  description,
  note,
  user_id
)
SELECT
  def_he.id,
  'import_hovedenhet_2025_selection',
  '2025-01-01'::date,
  'infinity'::date,
  'Import Job for BRREG Hovedenhet 2025 Selection',
  'This job handles the import of BRREG Hovedenhet selection data for 2025.',
  (SELECT id FROM public.user WHERE email = 'test.admin@statbus.org')
FROM def_he
ON CONFLICT (slug) DO NOTHING
RETURNING slug, state;
NOTICE:  identifier "import_hovedenhet_2025_selection_upload_check_state_before_insert" will be truncated to "import_hovedenhet_2025_selection_upload_check_state_before_inse"
NOTICE:  identifier "import_hovedenhet_2025_selection_upload_update_state_after_insert" will be truncated to "import_hovedenhet_2025_selection_upload_update_state_after_inse"
               slug               |       state        
----------------------------------+--------------------
 import_hovedenhet_2025_selection | waiting_for_upload
(1 row)

\echo "Load LU data FIRST"
"Load LU data FIRST"
\copy public.import_hovedenhet_2025_selection_upload FROM 'samples/norway/legal_unit/enheter-selection.csv' WITH CSV HEADER
\echo "Create import job for ES (underenhet) - uploaded SECOND"
"Create import job for ES (underenhet) - uploaded SECOND"
WITH def_ue AS (
  SELECT id FROM public.import_definition WHERE slug = 'brreg_underenhet_2024'
)
INSERT INTO public.import_job (
  definition_id,
  slug,
  default_valid_from,
  default_valid_to,
  description,
  note,
  user_id
)
SELECT
  def_ue.id,
  'import_underenhet_2025_selection',
  '2025-01-01'::date,
  'infinity'::date,
  'Import Job for BRREG Underenhet 2025 Selection',
  'This job handles the import of BRREG Underenhet selection data for 2025.',
  (SELECT id FROM public.user WHERE email = 'test.admin@statbus.org')
FROM def_ue
ON CONFLICT (slug) DO NOTHING
RETURNING slug, state;
NOTICE:  identifier "import_underenhet_2025_selection_upload_check_state_before_insert" will be truncated to "import_underenhet_2025_selection_upload_check_state_before_inse"
NOTICE:  identifier "import_underenhet_2025_selection_upload_update_state_after_insert" will be truncated to "import_underenhet_2025_selection_upload_update_state_after_inse"
               slug               |       state        
----------------------------------+--------------------
 import_underenhet_2025_selection | waiting_for_upload
(1 row)

\echo "Load ES data SECOND (lower priority than LU)"
"Load ES data SECOND (lower priority than LU)"
\copy public.import_underenhet_2025_selection_upload FROM 'samples/norway/establishment/underenheter-selection.csv' WITH CSV HEADER
\echo "Check import job state before processing"
"Check import job state before processing"
SELECT slug, state, total_rows, imported_rows
  FROM public.import_job
 WHERE slug LIKE 'import_%_selection'
 ORDER BY slug;
               slug               |      state       | total_rows | imported_rows 
----------------------------------+------------------+------------+---------------
 import_hovedenhet_2025_selection | upload_completed |       4924 |             0
 import_underenhet_2025_selection | upload_completed |      24027 |             0
(2 rows)

COMMIT;
-- ============================================================================
-- PHASE 2: PROCESSING (outside transaction - worker commits per task)
-- ============================================================================
\echo "Processing import jobs (LU processes first due to upload order)"
"Processing import jobs (LU processes first due to upload order)"
CALL worker.process_tasks(p_queue => 'import');
-- ============================================================================
-- PHASE 3: VERIFICATION AND BENCHMARK
-- ============================================================================
BEGIN;
\echo 'Check the states of the import job tasks'
Check the states of the import job tasks
SELECT queue, t.command, state, error
  FROM worker.tasks AS t
  JOIN worker.command_registry AS c on t.command = c.command
 WHERE t.command = 'import_job_process'
 ORDER BY priority;
 queue  |      command       |   state   | error 
--------+--------------------+-----------+-------
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
 import | import_job_process | completed | 
(97 rows)

\echo 'Check import job state after processing (deterministic)'
Check import job state after processing (deterministic)
SELECT slug, state, error IS NOT NULL AS failed, total_rows, imported_rows, import_completed_pct
  FROM public.import_job
 WHERE slug LIKE 'import_%_selection'
 ORDER BY slug;
               slug               |  state   | failed | total_rows | imported_rows | import_completed_pct 
----------------------------------+----------+--------+------------+---------------+----------------------
 import_hovedenhet_2025_selection | finished | f      |       4924 |          4924 |               100.00
 import_underenhet_2025_selection | finished | f      |      24027 |         24026 |               100.00
(2 rows)

\echo 'Check data row states after import'
Check data row states after import
SELECT state, count(*) FROM public.import_hovedenhet_2025_selection_data GROUP BY state ORDER BY state;
   state   | count 
-----------+-------
 processed |  4924
(1 row)

SELECT state, count(*) FROM public.import_underenhet_2025_selection_data GROUP BY state ORDER BY state;
   state   | count 
-----------+-------
 processed | 24026
 error     |     1
(2 rows)

\echo 'Show any error rows from import data tables (first 10)'
Show any error rows from import data tables (first 10)
SELECT row_id, errors, merge_status FROM public.import_hovedenhet_2025_selection_data WHERE state = 'error' ORDER BY row_id LIMIT 10;
 row_id | errors | merge_status 
--------+--------+--------------
(0 rows)

SELECT row_id, errors, merge_status FROM public.import_underenhet_2025_selection_data WHERE state = 'error' ORDER BY row_id LIMIT 10;
 row_id |                                                          errors                                                           | merge_status 
--------+---------------------------------------------------------------------------------------------------------------------------+--------------
  21817 | {"physical_country_iso_2_raw": "Country is required and must be valid when other physical address details are provided."} | {}
(1 row)

-- ============================================================================
-- SCALING ANALYSIS (Deterministic classification)
-- ============================================================================
\echo ''

\echo '--- Scaling Analysis (Deterministic) ---'
--- Scaling Analysis (Deterministic) ---
\echo 'Comparing LU (~5K rows) vs ES (~24K rows) performance.'
Comparing LU (~5K rows) vs ES (~24K rows) performance.
\echo 'ES has ~5x more rows - if time ratio >> 5, scaling is non-linear.'
ES has ~5x more rows - if time ratio >> 5, scaling is non-linear.
\echo ''

-- Calculate scaling classification (deterministic output only)
-- Variable timing data is written to the .perf file below
WITH job_metrics AS (
    SELECT
        slug,
        CASE WHEN slug LIKE '%hovedenhet%' THEN 'LU' ELSE 'ES' END AS phase,
        total_rows,
        analysis_rows_per_sec,
        import_rows_per_sec AS processing_rows_per_sec
    FROM public.import_job
    WHERE slug LIKE 'import_%_selection'
)
SELECT
    phase,
    total_rows,
    -- Classify based on absolute performance (deterministic categories)
    CASE
        WHEN processing_rows_per_sec >= 1000 THEN 'GOOD'
        WHEN processing_rows_per_sec >= 100 THEN 'OK'
        WHEN processing_rows_per_sec >= 10 THEN 'SLOW'
        ELSE 'VERY_SLOW'
    END AS proc_status,
    CASE
        WHEN analysis_rows_per_sec >= 1000 THEN 'GOOD'
        WHEN analysis_rows_per_sec >= 100 THEN 'OK'
        WHEN analysis_rows_per_sec >= 10 THEN 'SLOW'
        ELSE 'VERY_SLOW'
    END AS analysis_status
FROM job_metrics
ORDER BY phase;
 phase | total_rows | proc_status | analysis_status 
-------+------------+-------------+-----------------
 ES    |      24027 | OK          | GOOD
 LU    |       4924 | OK          | GOOD
(2 rows)

\echo ''

\echo 'Target: sql_saga achieves ~5000 rows/sec. Anything below 100 rows/sec needs investigation.'
Target: sql_saga achieves ~5000 rows/sec. Anything below 100 rows/sec needs investigation.
\echo 'See test/expected/performance/401_import_benchmark.perf for detailed timing.'
See test/expected/performance/401_import_benchmark.perf for detailed timing.
\echo ''

COMMIT;
-- ============================================================================
-- WRITE PERFORMANCE DATA TO FILE (Variable timing)
-- ============================================================================
BEGIN;
\set perf_file test/expected/performance/401_import_benchmark.perf
\pset tuples_only on
\pset footer off
\o :perf_file
SELECT '# Import Benchmark: BRREG Selection Data (~29K rows)';
SELECT '# These numbers are reference baselines, not test assertions.';
SELECT '# Target: sql_saga achieves ~5000 rows/sec at 1M+ scale.';
SELECT '#';
SELECT '';
\pset tuples_only off
SELECT '# Job timing summary:' as "header";
SELECT
    slug,
    total_rows,
    ROUND(EXTRACT(EPOCH FROM (analysis_stop_at - analysis_start_at))::numeric, 2) AS analysis_sec,
    ROUND(EXTRACT(EPOCH FROM (processing_stop_at - processing_start_at))::numeric, 2) AS processing_sec,
    ROUND(EXTRACT(EPOCH FROM (processing_stop_at - analysis_start_at))::numeric, 2) AS total_sec,
    ROUND(analysis_rows_per_sec::numeric, 2) AS analysis_rows_per_sec,
    ROUND(import_rows_per_sec::numeric, 2) AS processing_rows_per_sec
FROM public.import_job
WHERE slug LIKE 'import_%_selection'
ORDER BY slug;
\pset tuples_only on
SELECT '';
\pset tuples_only off
SELECT '# Per-row timing (ms/row):' as "header";
SELECT
    slug,
    total_rows,
    ROUND((EXTRACT(EPOCH FROM (analysis_stop_at - analysis_start_at)) * 1000 / NULLIF(total_rows, 0))::numeric, 2) AS analysis_ms_per_row,
    ROUND((EXTRACT(EPOCH FROM (processing_stop_at - processing_start_at)) * 1000 / NULLIF(total_rows, 0))::numeric, 2) AS processing_ms_per_row
FROM public.import_job
WHERE slug LIKE 'import_%_selection'
ORDER BY slug;
\pset tuples_only on
SELECT '';
\pset tuples_only off
SELECT '# Scaling comparison (LU ~5K vs ES ~24K rows):' as "header";
WITH metrics AS (
    SELECT
        CASE WHEN slug LIKE '%hovedenhet%' THEN 'LU' ELSE 'ES' END AS phase,
        total_rows,
        import_rows_per_sec AS processing_rows_per_sec,
        analysis_rows_per_sec,
        EXTRACT(EPOCH FROM (processing_stop_at - processing_start_at)) * 1000 / NULLIF(total_rows, 0) AS processing_ms_per_row
    FROM public.import_job
    WHERE slug LIKE 'import_%_selection'
)
SELECT
    'LU' AS base_phase,
    (SELECT total_rows FROM metrics WHERE phase = 'LU') AS lu_rows,
    (SELECT total_rows FROM metrics WHERE phase = 'ES') AS es_rows,
    ROUND((SELECT total_rows FROM metrics WHERE phase = 'ES')::numeric / 
          NULLIF((SELECT total_rows FROM metrics WHERE phase = 'LU'), 0), 2) AS row_ratio,
    ROUND((SELECT processing_ms_per_row FROM metrics WHERE phase = 'ES')::numeric / 
          NULLIF((SELECT processing_ms_per_row FROM metrics WHERE phase = 'LU'), 0), 2) AS ms_per_row_ratio,
    CASE
        WHEN (SELECT processing_ms_per_row FROM metrics WHERE phase = 'ES') / 
             NULLIF((SELECT processing_ms_per_row FROM metrics WHERE phase = 'LU'), 0) < 1.5 THEN 'LINEAR (O(n))'
        WHEN (SELECT processing_ms_per_row FROM metrics WHERE phase = 'ES') / 
             NULLIF((SELECT processing_ms_per_row FROM metrics WHERE phase = 'LU'), 0) < 3.0 THEN 'SUBLINEAR'
        ELSE 'NON-LINEAR (investigate)'
    END AS scaling_assessment;
\o
\pset footer on
\pset tuples_only off
COMMIT;
-- ============================================================================
-- PHASE 3B: ANALYTICS (enabled for performance testing)
-- ============================================================================
-- The analytics derivation includes multiple phases:
-- 1. derive_reports (~1ms) - just enqueues derive_statistical_history
-- 2. derive_statistical_history (~26s) - then enqueues derive_statistical_unit_facet
-- 3. derive_statistical_unit_facet (~1.2s) - then enqueues derive_statistical_history_facet
-- 4. derive_statistical_history_facet (~10+ min) - the bottleneck
BEGIN;
\echo Check the state of all tasks before running analytics.
Check the state of all tasks before running analytics.
SELECT queue, state, count(*) FROM worker.tasks AS t JOIN worker.command_registry AS c ON t.command = c.command WHERE c.queue != 'maintenance' GROUP BY queue,state ORDER BY queue,state;
   queue   |   state   | count 
-----------+-----------+-------
 analytics | pending   |     1
 import    | completed |    97
(2 rows)

COMMIT;
-- Run analytics outside transaction
CALL worker.process_tasks(p_queue => 'analytics');
BEGIN;
\echo Check the state of all tasks after running analytics.
Check the state of all tasks after running analytics.
SELECT queue, state, count(*) FROM worker.tasks AS t JOIN worker.command_registry AS c ON t.command = c.command WHERE c.queue != 'maintenance' GROUP BY queue,state ORDER BY queue,state;
   queue   |   state   | count 
-----------+-----------+-------
 analytics | completed |   204
 import    | completed |    97
(2 rows)

COMMIT;
-- Run any remaining tasks outside transaction
CALL worker.process_tasks();
BEGIN;
\echo Check the state of all tasks after final processing.
Check the state of all tasks after final processing.
SELECT queue, state, count(*) FROM worker.tasks AS t JOIN worker.command_registry AS c ON t.command = c.command WHERE c.queue != 'maintenance' GROUP BY queue,state ORDER BY queue,state;
   queue   |   state   | count 
-----------+-----------+-------
 analytics | completed |   204
 import    | completed |    97
(2 rows)

COMMIT;
-- ============================================================================
-- PHASE 4: CLEANUP
-- ============================================================================
\echo 'Resuming background worker'
Resuming background worker
SELECT worker.resume();
 resume 
--------
 
(1 row)

\i test/cleanup_unless_persist_is_specified.sql
\echo -- test/cleanup_unless_persist_is_specified.sql output suppressed
-- test/cleanup_unless_persist_is_specified.sql output suppressed
\set ECHO none
Isolated test database - skipping cleanup (database will be dropped)
