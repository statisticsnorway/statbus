--
-- Test: Import Performance Benchmark with Query Profiling
--
-- Uses small-history data (~40 rows) for quick iteration.
-- Enables AUTO EXPLAIN and pg_stat_monitor to identify slow queries.
--
-- Output files:
--   test/expected/performance/401_import_benchmark.perf - timing summary
--   test/expected/performance/401_import_benchmark_queries.perf - slow query analysis
--
-- ============================================================================
-- PHASE 0: SETUP AND ENABLE PROFILING
-- ============================================================================
-- Note: No reset needed - 4xx tests run in isolated databases from template
\echo 'Pausing background worker for test duration'
Pausing background worker for test duration
SELECT worker.pause('1 hour'::interval);
 pause 
-------
 
(1 row)

-- Enable AUTO EXPLAIN for slow queries (logs to PostgreSQL log)
-- This captures EXPLAIN ANALYZE for queries taking > 100ms
LOAD 'auto_explain';
SET auto_explain.log_min_duration = '100ms';
SET auto_explain.log_analyze = true;
SET auto_explain.log_buffers = true;
SET auto_explain.log_timing = true;
SET auto_explain.log_nested_statements = true;
-- Reset pg_stat_monitor if available (for query statistics)
DO $$
BEGIN
    IF EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_monitor') THEN
        PERFORM pg_stat_monitor_reset();
    END IF;
END;
$$;
-- ============================================================================
-- PHASE 1: SETUP (in transaction)
-- ============================================================================
BEGIN;
\i test/setup.sql
-- While the datestyle is set for the database, the pg_regress tool sets the MDY format
-- to ensure consistent date formatting, so we must manually override this
SET datestyle TO 'ISO, DMY';
\if :{?DEBUG}
SET client_min_messages TO debug1;
\else
SET client_min_messages TO NOTICE;
\endif
-- Create temporary function to execute queries as system user
CREATE OR REPLACE FUNCTION test.sudo_exec(
    sql text,
    OUT results jsonb
) RETURNS jsonb
SECURITY DEFINER LANGUAGE plpgsql AS $sudo_exec$
DECLARE
    result_rows jsonb;
BEGIN
    -- Check if the SQL starts with common DDL keywords
    IF sql ~* '^\s*(CREATE|DROP|ALTER|TRUNCATE|GRANT|REVOKE|ANALYZE)' THEN
        -- For DDL statements, execute directly
        EXECUTE sql;
        results := '[]'::jsonb;
    ELSE
        -- For DML/queries, wrap in a SELECT to capture results
        EXECUTE format('
            SELECT COALESCE(
                jsonb_agg(row_to_json(t)),
                ''[]''::jsonb
            )
            FROM (%s) t',
            sql
        ) INTO result_rows;
        results := result_rows;
    END IF;
END;
$sudo_exec$;
-- Grant execute to public since this is for testing
GRANT EXECUTE ON FUNCTION test.sudo_exec(text) TO PUBLIC;
\echo Add users for testing purposes
Add users for testing purposes
SELECT * FROM public.user_create(p_display_name => 'Test Admin', p_email => 'test.admin@statbus.org', p_statbus_role => 'admin_user'::statbus_role, p_password => 'Admin#123!');
         email          |  password  
------------------------+------------
 test.admin@statbus.org | Admin#123!
(1 row)

SELECT * FROM public.user_create(p_display_name => 'Test Regular', p_email => 'test.regular@statbus.org', p_statbus_role => 'regular_user'::statbus_role, p_password => 'Regular#123!');
          email           |   password   
--------------------------+--------------
 test.regular@statbus.org | Regular#123!
(1 row)

SELECT * FROM public.user_create(p_display_name => 'Test Restricted', p_email => 'test.restricted@statbus.org', p_statbus_role => 'restricted_user'::statbus_role, p_password => 'Restricted#123!');
            email            |    password     
-----------------------------+-----------------
 test.restricted@statbus.org | Restricted#123!
(1 row)

CREATE OR REPLACE PROCEDURE test.remove_pg_temp_for_tx_user_switch(p_keep_tables text[] DEFAULT '{}')
LANGUAGE plpgsql
AS $remove_pg_temp_for_tx_user_switch$
DECLARE
    rec record;
    v_found_count integer := 0;
BEGIN
    RAISE DEBUG 'Running test.remove_pg_temp_for_tx_user_switch(p_keep_tables => %)...', p_keep_tables;
    -- Remove temporary cache tables used by import, as we switch user inside the *same* transaction,
    -- and the new user can not modify tables owned by the previous import.
    -- This generic loop cleans up all tables and views in the pg_temp schema, except those specified to keep.
    FOR rec IN
        SELECT
            c.relname,
            c.relkind
        FROM pg_catalog.pg_class AS c
        LEFT JOIN pg_catalog.pg_namespace AS n ON n.oid = c.relnamespace
        WHERE c.relkind IN ('r', 'p', 'v', 'm') AND n.oid = pg_my_temp_schema() -- r=table, p=partitioned, v=view, m=materialized
          AND c.relname <> ALL(p_keep_tables)
    LOOP
        v_found_count := v_found_count + 1;
        IF rec.relkind IN ('r', 'p', 'm') THEN
            RAISE DEBUG '  -> Dropping temp TABLE %', rec.relname;
            EXECUTE format('DROP TABLE IF EXISTS pg_temp.%I CASCADE', rec.relname);
        ELSIF rec.relkind = 'v' THEN
            RAISE DEBUG '  -> Dropping temp VIEW %', rec.relname;
            EXECUTE format('DROP VIEW IF EXISTS pg_temp.%I CASCADE', rec.relname);
        END IF;
    END LOOP;

    RAISE DEBUG '...finished test.remove_pg_temp_for_tx_user_switch(). Found and dropped % objects.', v_found_count;

    -- This procedure is part of the sql_saga extension and has its own cleanup logic.
    -- While the loop above handles tables/views, this call ensures any other temporary
    -- objects it creates are also cleaned up.
    CALL sql_saga.temporal_merge_drop_temp_tables();
END;
$remove_pg_temp_for_tx_user_switch$;
CALL test.set_user_from_email('test.admin@statbus.org');
\echo 'Setting up Statbus for Norway'
Setting up Statbus for Norway
\i samples/norway/getting-started.sql
\i samples/norway/settings.sql
INSERT INTO settings(activity_category_standard_id,country_id)
SELECT (SELECT id FROM activity_category_standard WHERE code = 'nace_v2.1')
     , (SELECT id FROM public.country WHERE iso_2 = 'NO')
ON CONFLICT (only_one_setting)
DO UPDATE SET
   activity_category_standard_id = EXCLUDED.activity_category_standard_id,
   country_id = EXCLUDED.country_id
   WHERE settings.only_one_setting = EXCLUDED.only_one_setting;
;
\i samples/norway/activity_category/activity_category_norway.sql
\copy public.activity_category_available_custom FROM 'samples/norway/activity_category/activity_category_norway.csv' WITH (FORMAT csv, DELIMITER ',', QUOTE '"', HEADER true);
\i samples/norway/regions/norway-regions-2024.sql
\copy public.region_upload(path, name) FROM 'samples/norway/regions/norway-regions-2024.csv' WITH (FORMAT csv, DELIMITER ',', QUOTE '"', HEADER true);
\i samples/norway/sector/sector_norway.sql
\copy public.sector_custom_only FROM 'samples/norway/sector/sector_norway.csv' WITH (FORMAT csv, DELIMITER ',', QUOTE '"', HEADER true);
\i samples/norway/legal_form/legal_form_norway.sql
\copy public.legal_form_custom_only FROM 'samples/norway/legal_form/legal_form_norway.csv' WITH (FORMAT csv, DELIMITER ',', QUOTE '"', HEADER true);
\i samples/norway/data_source/data_source_norway.sql
\copy public.data_source_custom (code, name) FROM 'samples/norway/data_source/data_source_norway.csv' WITH (FORMAT csv, DELIMITER ',', QUOTE '"', HEADER true);
\i samples/norway/brreg/create-import-definition-hovedenhet-2024.sql
-- Create import definition for BRREG Hovedenhet (legal_unit) using 2024 columns
-- This version uses the new import system with steps and helper functions.
DO $$
DECLARE
    def_id INT;
    -- Define the steps needed for a legal unit import with a time context
    lu_steps TEXT[] := ARRAY[
        'external_idents', 'data_source', 'enterprise_link_for_legal_unit', 'valid_time', 'status', 'legal_unit',
        'physical_location', 'postal_location', 'primary_activity', 'secondary_activity',
        'contact', 'statistical_variables', 'tags', 'edit_info', 'metadata'
    ];
    map_rec RECORD;
    v_source_col_id INT;
    v_data_col_id INT;
BEGIN
    -- 1. Create the definition record (initially invalid)
    INSERT INTO public.import_definition (slug, name, note, strategy, mode, valid_time_from, valid, data_source_id)
    VALUES ('brreg_hovedenhet_2024', 'Import of BRREG Hovedenhet using 2024 columns', 'Easy upload of the CSV file found at brreg.', 'insert_or_update', 'legal_unit', 'job_provided', false, (SELECT id FROM public.data_source WHERE code = 'brreg'))
    RETURNING id INTO def_id;

    -- 2. Link the required steps to the definition
    PERFORM import.link_steps_to_definition(def_id, lu_steps);

    -- 3. Data columns are defined per step and generated by lifecycle callbacks.
    --    The call to admin.create_data_columns_for_definition(def_id) is removed.

    -- 4. Define source columns and mappings declaratively
    -- Create a temporary table to hold the mapping list
    CREATE TEMP TABLE temp_mapping_list (
        priority INT,
        source_name TEXT,
        target_name TEXT
    ) ON COMMIT DROP;

    -- Populate the temporary table
    INSERT INTO temp_mapping_list (priority, source_name, target_name)
    SELECT ROW_NUMBER() OVER () as priority, source_name, target_name
    FROM (VALUES
            ('organisasjonsnummer', 'tax_ident'),
            ('navn', 'name'),
            ('organisasjonsform.kode', 'legal_form_code'),
            ('organisasjonsform.beskrivelse', NULL), -- Ignored
            ('naeringskode1.kode', 'primary_activity_category_code'),
            ('naeringskode1.beskrivelse', NULL), -- Ignored
            ('naeringskode2.kode', 'secondary_activity_category_code'),
            ('naeringskode2.beskrivelse', NULL), -- Ignored
            ('naeringskode3.kode', NULL), -- Ignored
            ('naeringskode3.beskrivelse', NULL), -- Ignored
            ('hjelpeenhetskode.kode', NULL), -- Ignored
            ('hjelpeenhetskode.beskrivelse', NULL), -- Ignored
            ('harRegistrertAntallAnsatte', NULL), -- Ignored
            ('antallAnsatte', 'employees'),
            ('hjemmeside', 'web_address'),
            ('postadresse.adresse', 'postal_address_part1'),
            ('postadresse.poststed', 'postal_postplace'),
            ('postadresse.postnummer', 'postal_postcode'),
            ('postadresse.kommune', NULL), -- Ignored
            ('postadresse.kommunenummer', 'postal_region_code'),
            ('postadresse.land', NULL), -- Ignored
            ('postadresse.landkode', 'postal_country_iso_2'),
            ('forretningsadresse.adresse', 'physical_address_part1'),
            ('forretningsadresse.poststed', 'physical_postplace'),
            ('forretningsadresse.postnummer', 'physical_postcode'),
            ('forretningsadresse.kommune', NULL), -- Ignored
            ('forretningsadresse.kommunenummer', 'physical_region_code'),
            ('forretningsadresse.land', NULL), -- Ignored
            ('forretningsadresse.landkode', 'physical_country_iso_2'),
            ('institusjonellSektorkode.kode', 'sector_code'),
            ('institusjonellSektorkode.beskrivelse', NULL), -- Ignored
            ('sisteInnsendteAarsregnskap', NULL), -- Ignored
            ('registreringsdatoenhetsregisteret', NULL), -- Ignored
            ('stiftelsesdato', 'birth_date'),
            ('registrertIMvaRegisteret', NULL), -- Ignored
            ('frivilligMvaRegistrertBeskrivelser', NULL), -- Ignored
            ('registrertIFrivillighetsregisteret', NULL), -- Ignored
            ('registrertIForetaksregisteret', NULL), -- Ignored
            ('registrertIStiftelsesregisteret', NULL), -- Ignored
            ('konkurs', NULL), -- Ignored
            ('konkursdato', NULL), -- Ignored
            ('underAvvikling', NULL), -- Ignored
            ('underAvviklingDato', NULL), -- Ignored
            ('underTvangsavviklingEllerTvangsopplosning', NULL), -- Ignored
            ('tvangsopplostPgaManglendeDagligLederDato', NULL), -- Ignored
            ('tvangsopplostPgaManglendeRevisorDato', NULL), -- Ignored
            ('tvangsopplostPgaManglendeRegnskapDato', NULL), -- Ignored
            ('tvangsopplostPgaMangelfulltStyreDato', NULL), -- Ignored
            ('tvangsavvikletPgaManglendeSlettingDato', NULL), -- Ignored
            ('overordnetEnhet', NULL), -- Ignored (Handled by link step if needed)
            ('maalform', NULL), -- Ignored
            ('vedtektsdato', NULL), -- Ignored
            ('vedtektsfestetFormaal', NULL), -- Ignored
            ('aktivitet', NULL) -- Ignored
        ) AS v(source_name, target_name);

    -- Loop through the temporary mapping list table
    FOR map_rec IN SELECT * FROM temp_mapping_list ORDER BY priority LOOP
        -- Insert the source column
        INSERT INTO public.import_source_column (definition_id, column_name, priority)
        VALUES (def_id, map_rec.source_name, map_rec.priority)
        RETURNING id INTO v_source_col_id;

        -- If a target is specified, create the mapping
        IF map_rec.target_name IS NOT NULL THEN
            -- Find the corresponding data column ID by joining through the steps linked to this definition
            SELECT dc.id INTO v_data_col_id
            FROM public.import_definition_step ds
            JOIN public.import_data_column dc ON ds.step_id = dc.step_id
            WHERE ds.definition_id = def_id
              AND dc.column_name = map_rec.target_name || '_raw'
              AND dc.purpose = 'source_input';

            IF v_data_col_id IS NOT NULL THEN
                -- Insert the mapping
                INSERT INTO public.import_mapping (definition_id, source_column_id, target_data_column_id, target_data_column_purpose, is_ignored)
                VALUES (def_id, v_source_col_id, v_data_col_id, 'source_input'::public.import_data_column_purpose, FALSE)
                ON CONFLICT (definition_id, source_column_id, target_data_column_id) DO NOTHING;
            ELSE
                -- If target_name was specified but no data_col_id found, or if target_name was NULL initially.
                INSERT INTO public.import_mapping (definition_id, source_column_id, is_ignored, target_data_column_id, target_data_column_purpose)
                VALUES (def_id, v_source_col_id, TRUE, NULL, NULL)
                ON CONFLICT (definition_id, source_column_id, target_data_column_id) WHERE target_data_column_id IS NULL
                DO NOTHING;
            END IF;
        ELSE -- map_rec.target_name IS NULL
            INSERT INTO public.import_mapping (definition_id, source_column_id, is_ignored, target_data_column_id, target_data_column_purpose)
            VALUES (def_id, v_source_col_id, TRUE, NULL, NULL)
            ON CONFLICT (definition_id, source_column_id, target_data_column_id) WHERE target_data_column_id IS NULL
            DO NOTHING;
        END IF;
    END LOOP;

    -- 4c. Add mappings for default values (valid_from, valid_to) for job_provided definitions
    -- These source columns don't exist, so source_column_id is NULL
    DECLARE
        v_valid_time_step_id INT;
    BEGIN
        SELECT id INTO v_valid_time_step_id FROM public.import_step WHERE code = 'valid_time';

        INSERT INTO public.import_mapping (definition_id, source_expression, target_data_column_id)
        SELECT def_id, 'default'::public.import_source_expression, dc.id
        FROM public.import_data_column dc
        WHERE dc.step_id = v_valid_time_step_id
          AND dc.column_name IN ('valid_from_raw', 'valid_to_raw')
          AND dc.purpose = 'source_input'
        ON CONFLICT DO NOTHING;
    END;


    -- 5. Set the 'tax_ident' data column as uniquely identifying for the prepare step UPSERT
    DECLARE
        v_idents_step_id INT;
    BEGIN
        SELECT id INTO v_idents_step_id FROM public.import_step WHERE code = 'external_idents';
        UPDATE public.import_data_column
        SET is_uniquely_identifying = true
        WHERE step_id = v_idents_step_id -- Use step_id
          AND column_name = 'tax_ident_raw'
          AND purpose = 'source_input';
    END; -- End of the inner BEGIN/END block
    
    DROP TABLE temp_mapping_list;

END $$; -- End of the main DO block
-- Display the created definition details
SELECT d.slug,
       d.name,
       d.note,
       ds.code as data_source,
       d.valid_time_from,
       d.strategy,
       d.valid,
       d.validation_error
FROM public.import_definition d
LEFT JOIN public.data_source ds ON ds.id = d.data_source_id
WHERE d.slug = 'brreg_hovedenhet_2024';
         slug          |                     name                      |                    note                     | data_source | valid_time_from |     strategy     | valid | validation_error 
-----------------------+-----------------------------------------------+---------------------------------------------+-------------+-----------------+------------------+-------+------------------
 brreg_hovedenhet_2024 | Import of BRREG Hovedenhet using 2024 columns | Easy upload of the CSV file found at brreg. | brreg       | job_provided    | insert_or_update | t     | 
(1 row)

-- Set the definition to valid (can now be used to create jobs)
UPDATE public.import_definition
SET valid = true, validation_error = NULL
WHERE slug = 'brreg_hovedenhet_2024';
\i samples/norway/brreg/create-import-definition-underenhet-2024.sql
-- Create import definition for BRREG Underenhet (establishment for legal unit) using 2024 columns
-- This version uses the new import system with steps and helper functions.
DO $$
DECLARE
    def_id INT;
    -- Define the steps needed for an establishment linked to a legal unit import with a time context
    es_steps TEXT[] := ARRAY[
        'external_idents', 'data_source', 'link_establishment_to_legal_unit', 'valid_time', 'status', 'establishment',
        'physical_location', 'postal_location', 'primary_activity', 'secondary_activity',
        'contact', 'statistical_variables', 'tags', 'edit_info', 'metadata'
    ];
    map_rec RECORD;
    v_source_col_id INT;
    v_data_col_id INT;
BEGIN
    -- 1. Create the definition record (initially invalid)
    INSERT INTO public.import_definition (slug, name, note, strategy, mode, valid_time_from, valid, data_source_id)
    VALUES ('brreg_underenhet_2024', 'Import of BRREG Underenhet using 2024 columns', 'Easy upload of the CSV file found at brreg.', 'insert_or_update', 'establishment_formal', 'job_provided', false, (SELECT id FROM public.data_source WHERE code = 'brreg'))
    RETURNING id INTO def_id;

    -- 2. Link the required steps to the definition
    PERFORM import.link_steps_to_definition(def_id, es_steps);

    -- 3. Data columns are defined per step and generated by lifecycle callbacks.
    --    The call to admin.create_data_columns_for_definition(def_id) is removed.

    -- 4. Define source columns and mappings declaratively
    -- Create a temporary table to hold the mapping list
    CREATE TEMP TABLE temp_mapping_list (
        priority INT,
        source_name TEXT,
        target_name TEXT
    ) ON COMMIT DROP;

    -- Populate the temporary table
    INSERT INTO temp_mapping_list (priority, source_name, target_name)
    SELECT ROW_NUMBER() OVER () as priority, source_name, target_name
    FROM (VALUES
            ('organisasjonsnummer', 'tax_ident'),
            ('navn', 'name'),
            ('organisasjonsform.kode', NULL), -- Ignored
            ('organisasjonsform.beskrivelse', NULL), -- Ignored
            ('naeringskode1.kode', 'primary_activity_category_code'),
            ('naeringskode1.beskrivelse', NULL), -- Ignored
            ('naeringskode2.kode', 'secondary_activity_category_code'),
            ('naeringskode2.beskrivelse', NULL), -- Ignored
            ('naeringskode3.kode', NULL), -- Ignored
            ('naeringskode3.beskrivelse', NULL), -- Ignored
            ('hjelpeenhetskode.kode', NULL), -- Ignored
            ('hjelpeenhetskode.beskrivelse', NULL), -- Ignored
            ('harRegistrertAntallAnsatte', NULL), -- Ignored
            ('antallAnsatte', 'employees'),
            ('hjemmeside', 'web_address'),
            ('postadresse.adresse', 'postal_address_part1'),
            ('postadresse.poststed', 'postal_postplace'),
            ('postadresse.postnummer', 'postal_postcode'),
            ('postadresse.kommune', NULL), -- Ignored
            ('postadresse.kommunenummer', 'postal_region_code'),
            ('postadresse.land', NULL), -- Ignored
            ('postadresse.landkode', 'postal_country_iso_2'),
            ('beliggenhetsadresse.adresse', 'physical_address_part1'),
            ('beliggenhetsadresse.poststed', 'physical_postplace'),
            ('beliggenhetsadresse.postnummer', 'physical_postcode'),
            ('beliggenhetsadresse.kommune', NULL), -- Ignored
            ('beliggenhetsadresse.kommunenummer', 'physical_region_code'),
            ('beliggenhetsadresse.land', NULL), -- Ignored
            ('beliggenhetsadresse.landkode', 'physical_country_iso_2'),
            ('registreringsdatoIEnhetsregisteret', NULL), -- Ignored
            ('frivilligMvaRegistrertBeskrivelser', NULL), -- Ignored
            ('registrertIMvaregisteret', NULL), -- Ignored
            ('oppstartsdato', 'birth_date'),
            ('datoEierskifte', NULL), -- Ignored
            ('overordnetEnhet', 'legal_unit_tax_ident'), -- Map to the dynamic column
            ('nedleggelsesdato', 'death_date')
        ) AS v(source_name, target_name);

    -- Loop through the temporary mapping list table
    FOR map_rec IN SELECT * FROM temp_mapping_list ORDER BY priority LOOP
        -- Insert the source column
        INSERT INTO public.import_source_column (definition_id, column_name, priority)
        VALUES (def_id, map_rec.source_name, map_rec.priority)
        RETURNING id INTO v_source_col_id;

        -- If a target is specified, create the mapping
        IF map_rec.target_name IS NOT NULL THEN
            -- Find the corresponding data column ID by joining through the steps linked to this definition
            SELECT dc.id INTO v_data_col_id
            FROM public.import_definition_step ds
            JOIN public.import_data_column dc ON ds.step_id = dc.step_id
            WHERE ds.definition_id = def_id
              AND dc.column_name = map_rec.target_name || '_raw'
              AND dc.purpose = 'source_input';

            IF v_data_col_id IS NOT NULL THEN
                -- Insert the mapping
                INSERT INTO public.import_mapping (definition_id, source_column_id, target_data_column_id, target_data_column_purpose, is_ignored)
                VALUES (def_id, v_source_col_id, v_data_col_id, 'source_input'::public.import_data_column_purpose, FALSE)
                ON CONFLICT (definition_id, source_column_id, target_data_column_id) DO NOTHING;
            ELSE
                -- If target_name was specified but no data_col_id found, or if target_name was NULL initially.
                INSERT INTO public.import_mapping (definition_id, source_column_id, is_ignored, target_data_column_id, target_data_column_purpose)
                VALUES (def_id, v_source_col_id, TRUE, NULL, NULL)
                ON CONFLICT (definition_id, source_column_id, target_data_column_id) WHERE target_data_column_id IS NULL
                DO NOTHING;
            END IF;
        ELSE -- map_rec.target_name IS NULL
            INSERT INTO public.import_mapping (definition_id, source_column_id, is_ignored, target_data_column_id, target_data_column_purpose)
            VALUES (def_id, v_source_col_id, TRUE, NULL, NULL)
            ON CONFLICT (definition_id, source_column_id, target_data_column_id) WHERE target_data_column_id IS NULL
            DO NOTHING;
        END IF;
    END LOOP;

    -- 4c. Add mappings for default values (valid_from, valid_to) for job_provided definitions
    -- These source columns don't exist, so source_column_id is NULL
    DECLARE
        v_valid_time_step_id INT;
    BEGIN
        SELECT id INTO v_valid_time_step_id FROM public.import_step WHERE code = 'valid_time';

        INSERT INTO public.import_mapping (definition_id, source_expression, target_data_column_id)
        SELECT def_id, 'default'::public.import_source_expression, dc.id
        FROM public.import_data_column dc
        WHERE dc.step_id = v_valid_time_step_id
          AND dc.column_name IN ('valid_from_raw', 'valid_to_raw')
          AND dc.purpose = 'source_input'
        ON CONFLICT DO NOTHING;
    END;


    -- 5. Set the 'tax_ident' data column as uniquely identifying for the prepare step UPSERT
    DECLARE
        v_idents_step_id INT;
    BEGIN
        SELECT id INTO v_idents_step_id FROM public.import_step WHERE code = 'external_idents';
        UPDATE public.import_data_column
        SET is_uniquely_identifying = true
        WHERE step_id = v_idents_step_id -- Use step_id
          AND column_name = 'tax_ident_raw'
          AND purpose = 'source_input';
    END; -- End of the inner BEGIN/END block

    DROP TABLE temp_mapping_list;
    
END $$; -- End of the main DO block
-- Display the created definition details
SELECT d.slug,
       d.name,
       d.note,
       ds.code as data_source,
       d.valid_time_from,
       d.strategy,
       d.valid,
       d.validation_error
FROM public.import_definition d
LEFT JOIN public.data_source ds ON ds.id = d.data_source_id
WHERE d.slug = 'brreg_underenhet_2024';
         slug          |                     name                      |                    note                     | data_source | valid_time_from |     strategy     | valid | validation_error 
-----------------------+-----------------------------------------------+---------------------------------------------+-------------+-----------------+------------------+-------+------------------
 brreg_underenhet_2024 | Import of BRREG Underenhet using 2024 columns | Easy upload of the CSV file found at brreg. | brreg       | job_provided    | insert_or_update | t     | 
(1 row)

-- Set the definition to valid (can now be used to create jobs)
UPDATE public.import_definition
SET valid = true, validation_error = NULL
WHERE slug = 'brreg_underenhet_2024';
-- Create benchmark results table
CREATE TEMP TABLE benchmark_results (
    phase TEXT NOT NULL,
    job_slug TEXT NOT NULL,
    total_rows INT,
    analysis_ms NUMERIC,
    processing_ms NUMERIC,
    total_ms NUMERIC,
    analysis_rows_per_sec NUMERIC,
    processing_rows_per_sec NUMERIC
) ON COMMIT PRESERVE ROWS;
-- Per year jobs for hovedenhet (LU)
WITH def AS (SELECT id FROM public.import_definition WHERE slug = 'brreg_hovedenhet_2024')
INSERT INTO public.import_job (definition_id, slug, default_valid_from, default_valid_to, description, note, edit_comment)
SELECT def.id, 'import_lu_2015_bench', '2015-01-01'::DATE, 'infinity'::DATE,
       'Benchmark LU 2015', 'Benchmark test', 'LU 2015'
FROM def;
WITH def AS (SELECT id FROM public.import_definition WHERE slug = 'brreg_hovedenhet_2024')
INSERT INTO public.import_job (definition_id, slug, default_valid_from, default_valid_to, description, note, edit_comment)
SELECT def.id, 'import_lu_2016_bench', '2016-01-01'::DATE, 'infinity'::DATE,
       'Benchmark LU 2016', 'Benchmark test', 'LU 2016'
FROM def;
WITH def AS (SELECT id FROM public.import_definition WHERE slug = 'brreg_hovedenhet_2024')
INSERT INTO public.import_job (definition_id, slug, default_valid_from, default_valid_to, description, note, edit_comment)
SELECT def.id, 'import_lu_2017_bench', '2017-01-01'::DATE, 'infinity'::DATE,
       'Benchmark LU 2017', 'Benchmark test', 'LU 2017'
FROM def;
WITH def AS (SELECT id FROM public.import_definition WHERE slug = 'brreg_hovedenhet_2024')
INSERT INTO public.import_job (definition_id, slug, default_valid_from, default_valid_to, description, note, edit_comment)
SELECT def.id, 'import_lu_2018_bench', '2018-01-01'::DATE, 'infinity'::DATE,
       'Benchmark LU 2018', 'Benchmark test', 'LU 2018'
FROM def;
-- Per year jobs for underenhet (ES)
WITH def AS (SELECT id FROM public.import_definition WHERE slug = 'brreg_underenhet_2024')
INSERT INTO public.import_job (definition_id, slug, default_valid_from, default_valid_to, description, note, edit_comment)
SELECT def.id, 'import_es_2015_bench', '2015-01-01'::DATE, 'infinity'::DATE,
       'Benchmark ES 2015', 'Benchmark test', 'ES 2015'
FROM def;
WITH def AS (SELECT id FROM public.import_definition WHERE slug = 'brreg_underenhet_2024')
INSERT INTO public.import_job (definition_id, slug, default_valid_from, default_valid_to, description, note, edit_comment)
SELECT def.id, 'import_es_2016_bench', '2016-01-01'::DATE, 'infinity'::DATE,
       'Benchmark ES 2016', 'Benchmark test', 'ES 2016'
FROM def;
WITH def AS (SELECT id FROM public.import_definition WHERE slug = 'brreg_underenhet_2024')
INSERT INTO public.import_job (definition_id, slug, default_valid_from, default_valid_to, description, note, edit_comment)
SELECT def.id, 'import_es_2017_bench', '2017-01-01'::DATE, 'infinity'::DATE,
       'Benchmark ES 2017', 'Benchmark test', 'ES 2017'
FROM def;
WITH def AS (SELECT id FROM public.import_definition WHERE slug = 'brreg_underenhet_2024')
INSERT INTO public.import_job (definition_id, slug, default_valid_from, default_valid_to, description, note, edit_comment)
SELECT def.id, 'import_es_2018_bench', '2018-01-01'::DATE, 'infinity'::DATE,
       'Benchmark ES 2018', 'Benchmark test', 'ES 2018'
FROM def;
-- Load all LU data first (upload order determines processing priority)
\echo 'Loading LU data (oldest first)'
Loading LU data (oldest first)
\copy public.import_lu_2015_bench_upload FROM 'samples/norway/small-history/2015-enheter.csv' WITH CSV HEADER
\copy public.import_lu_2016_bench_upload FROM 'samples/norway/small-history/2016-enheter.csv' WITH CSV HEADER
\copy public.import_lu_2017_bench_upload FROM 'samples/norway/small-history/2017-enheter.csv' WITH CSV HEADER
\copy public.import_lu_2018_bench_upload FROM 'samples/norway/small-history/2018-enheter.csv' WITH CSV HEADER
-- Load all ES data after LU (ES depends on LU external_idents)
\echo 'Loading ES data (oldest first, AFTER all LU)'
Loading ES data (oldest first, AFTER all LU)
\copy public.import_es_2015_bench_upload FROM 'samples/norway/small-history/2015-underenheter.csv' WITH CSV HEADER
\copy public.import_es_2016_bench_upload FROM 'samples/norway/small-history/2016-underenheter.csv' WITH CSV HEADER
\copy public.import_es_2017_bench_upload FROM 'samples/norway/small-history/2017-underenheter.csv' WITH CSV HEADER
\copy public.import_es_2018_bench_upload FROM 'samples/norway/small-history/2018-underenheter.csv' WITH CSV HEADER
\echo 'Jobs created and data loaded'
Jobs created and data loaded
SELECT slug, state, total_rows FROM public.import_job WHERE slug LIKE 'import_%_bench' ORDER BY slug;
         slug         |      state       | total_rows 
----------------------+------------------+------------
 import_es_2015_bench | upload_completed |          4
 import_es_2016_bench | upload_completed |          6
 import_es_2017_bench | upload_completed |          4
 import_es_2018_bench | upload_completed |          5
 import_lu_2015_bench | upload_completed |          4
 import_lu_2016_bench | upload_completed |          6
 import_lu_2017_bench | upload_completed |          5
 import_lu_2018_bench | upload_completed |          6
(8 rows)

COMMIT;
-- ============================================================================
-- PHASE 2: PROCESSING (outside transaction - worker commits per task)
-- ============================================================================
\echo ''

\echo '================================================================================'
================================================================================
\echo 'IMPORT BENCHMARK: Performance Measurement with Query Profiling'
IMPORT BENCHMARK: Performance Measurement with Query Profiling
\echo '================================================================================'
================================================================================
\echo ''

-- Process all import jobs
\echo 'Processing all import jobs...'
Processing all import jobs...
CALL worker.process_tasks(p_queue => 'import');
-- ============================================================================
-- PHASE 3: COLLECT BENCHMARK RESULTS
-- ============================================================================
BEGIN;
\echo ''

\echo 'Collecting benchmark results...'
Collecting benchmark results...
-- Insert timing data from import_job table
INSERT INTO benchmark_results (phase, job_slug, total_rows, analysis_ms, processing_ms, total_ms, analysis_rows_per_sec, processing_rows_per_sec)
SELECT
    CASE WHEN slug LIKE '%_lu_%' THEN 'LU' ELSE 'ES' END AS phase,
    slug,
    total_rows,
    ROUND(EXTRACT(EPOCH FROM (analysis_stop_at - analysis_start_at)) * 1000, 1) AS analysis_ms,
    ROUND(EXTRACT(EPOCH FROM (processing_stop_at - processing_start_at)) * 1000, 1) AS processing_ms,
    ROUND(EXTRACT(EPOCH FROM (processing_stop_at - analysis_start_at)) * 1000, 1) AS total_ms,
    analysis_rows_per_sec,
    import_rows_per_sec
FROM public.import_job
WHERE slug LIKE 'import_%_bench'
ORDER BY slug;
-- ============================================================================
-- DETERMINISTIC OUTPUT: Job states and row counts
-- ============================================================================
\echo ''

\echo '--- Job Completion Status (Deterministic) ---'
--- Job Completion Status (Deterministic) ---
SELECT
    slug,
    state,
    total_rows,
    imported_rows,
    CASE WHEN error IS NOT NULL THEN 'ERROR' ELSE 'OK' END AS status
FROM public.import_job
WHERE slug LIKE 'import_%_bench'
ORDER BY slug;
         slug         |  state   | total_rows | imported_rows | status 
----------------------+----------+------------+---------------+--------
 import_es_2015_bench | finished |          4 |             4 | OK
 import_es_2016_bench | finished |          6 |             6 | OK
 import_es_2017_bench | finished |          4 |             4 | OK
 import_es_2018_bench | finished |          5 |             5 | OK
 import_lu_2015_bench | finished |          4 |             4 | OK
 import_lu_2016_bench | finished |          6 |             6 | OK
 import_lu_2017_bench | finished |          5 |             5 | OK
 import_lu_2018_bench | finished |          6 |             6 | OK
(8 rows)

\echo ''

\echo '--- Data Row States (Deterministic) ---'
--- Data Row States (Deterministic) ---
SELECT state, count(*) AS count FROM public.import_lu_2015_bench_data GROUP BY state ORDER BY state;
   state   | count 
-----------+-------
 processed |     4
(1 row)

SELECT state, count(*) AS count FROM public.import_lu_2016_bench_data GROUP BY state ORDER BY state;
   state   | count 
-----------+-------
 processed |     6
(1 row)

SELECT state, count(*) AS count FROM public.import_lu_2017_bench_data GROUP BY state ORDER BY state;
   state   | count 
-----------+-------
 processed |     5
(1 row)

SELECT state, count(*) AS count FROM public.import_lu_2018_bench_data GROUP BY state ORDER BY state;
   state   | count 
-----------+-------
 processed |     6
(1 row)

SELECT state, count(*) AS count FROM public.import_es_2015_bench_data GROUP BY state ORDER BY state;
   state   | count 
-----------+-------
 processed |     4
(1 row)

SELECT state, count(*) AS count FROM public.import_es_2016_bench_data GROUP BY state ORDER BY state;
   state   | count 
-----------+-------
 processed |     6
(1 row)

SELECT state, count(*) AS count FROM public.import_es_2017_bench_data GROUP BY state ORDER BY state;
   state   | count 
-----------+-------
 processed |     4
(1 row)

SELECT state, count(*) AS count FROM public.import_es_2018_bench_data GROUP BY state ORDER BY state;
   state   | count 
-----------+-------
 processed |     5
(1 row)

\echo ''

\echo '--- Error Rows (Deterministic) ---'
--- Error Rows (Deterministic) ---
SELECT row_id, state, errors FROM public.import_lu_2015_bench_data WHERE state = 'error' ORDER BY row_id;
 row_id | state | errors 
--------+-------+--------
(0 rows)

SELECT row_id, state, errors FROM public.import_es_2015_bench_data WHERE state = 'error' ORDER BY row_id;
 row_id | state | errors 
--------+-------+--------
(0 rows)

-- ============================================================================
-- SCALING ANALYSIS (Deterministic classification)
-- ============================================================================
\echo ''

\echo '--- Scaling Analysis (Deterministic) ---'
--- Scaling Analysis (Deterministic) ---
\echo 'Comparing LU vs ES performance to detect O(n²) regressions.'
Comparing LU vs ES performance to detect O(n²) regressions.
\echo ''

-- Calculate aggregate stats per phase
CREATE TEMP VIEW phase_summary AS
SELECT
    phase,
    SUM(total_rows) AS total_rows,
    SUM(analysis_ms) AS analysis_ms,
    SUM(processing_ms) AS processing_ms,
    SUM(total_ms) AS total_ms,
    CASE WHEN SUM(analysis_ms) > 0 THEN ROUND(SUM(total_rows) / (SUM(analysis_ms) / 1000.0), 1) END AS analysis_rows_per_sec,
    CASE WHEN SUM(processing_ms) > 0 THEN ROUND(SUM(total_rows) / (SUM(processing_ms) / 1000.0), 1) END AS processing_rows_per_sec
FROM benchmark_results
GROUP BY phase;
-- Output scaling classification (deterministic)
SELECT
    phase,
    total_rows,
    CASE
        WHEN processing_rows_per_sec IS NULL THEN 'NO_DATA'
        WHEN processing_rows_per_sec >= 100 THEN 'OK'
        WHEN processing_rows_per_sec >= 50 THEN 'SLOW'
        ELSE 'VERY_SLOW'
    END AS processing_status,
    CASE
        WHEN analysis_rows_per_sec IS NULL THEN 'NO_DATA'
        WHEN analysis_rows_per_sec >= 100 THEN 'OK'
        WHEN analysis_rows_per_sec >= 50 THEN 'SLOW'
        ELSE 'VERY_SLOW'
    END AS analysis_status
FROM phase_summary
ORDER BY phase;
 phase | total_rows | processing_status | analysis_status 
-------+------------+-------------------+-----------------
 ES    |         19 | VERY_SLOW         | VERY_SLOW
 LU    |         21 | VERY_SLOW         | VERY_SLOW
(2 rows)

\echo ''

\echo 'See test/expected/performance/401_import_benchmark.perf for detailed timing.'
See test/expected/performance/401_import_benchmark.perf for detailed timing.
\echo 'Check PostgreSQL logs for AUTO EXPLAIN output of slow queries.'
Check PostgreSQL logs for AUTO EXPLAIN output of slow queries.
\echo ''

COMMIT;
-- ============================================================================
-- WRITE PERFORMANCE DATA TO FILE (Variable timing)
-- ============================================================================
BEGIN;
\set perf_file test/expected/performance/401_import_benchmark.perf
\pset tuples_only on
\pset footer off
\o :perf_file
SELECT '# Import Benchmark Performance Baseline (Small History ~40 rows)';
SELECT '# With AUTO EXPLAIN enabled for queries > 100ms';
SELECT '# Check PostgreSQL logs for detailed query plans';
SELECT '#';
SELECT '';
\pset tuples_only off
SELECT '# Per-job timing:' as "header";
SELECT
    job_slug,
    total_rows,
    ROUND(analysis_ms)::int AS analysis_ms,
    ROUND(processing_ms)::int AS processing_ms,
    ROUND(total_ms)::int AS total_ms,
    ROUND(analysis_rows_per_sec, 1) AS analysis_rows_per_sec,
    ROUND(processing_rows_per_sec, 1) AS processing_rows_per_sec
FROM benchmark_results
ORDER BY job_slug;
\pset tuples_only on
SELECT '';
\pset tuples_only off
SELECT '# Phase summary:' as "header";
SELECT
    phase,
    total_rows,
    ROUND(analysis_ms)::int AS analysis_ms,
    ROUND(processing_ms)::int AS processing_ms,
    ROUND(total_ms)::int AS total_ms,
    ROUND(analysis_rows_per_sec, 1) AS analysis_rows_per_sec,
    ROUND(processing_rows_per_sec, 1) AS processing_rows_per_sec
FROM phase_summary
ORDER BY phase;
\pset tuples_only on
SELECT '';
\pset tuples_only off
SELECT '# Detailed job info from import_job table:' as "header";
SELECT
    slug,
    state,
    total_rows,
    imported_rows,
    ROUND(analysis_rows_per_sec, 2) AS analysis_rows_per_sec,
    ROUND(import_rows_per_sec, 2) AS import_rows_per_sec,
    ROUND(EXTRACT(EPOCH FROM (analysis_stop_at - analysis_start_at)), 2) AS analysis_sec,
    ROUND(EXTRACT(EPOCH FROM (processing_stop_at - processing_start_at)), 2) AS processing_sec
FROM public.import_job
WHERE slug LIKE 'import_%_bench'
ORDER BY slug;
\o
\pset footer on
\pset tuples_only off
COMMIT;
-- ============================================================================
-- COLLECT pg_stat_monitor DATA (if available)
-- ============================================================================
BEGIN;
\set queries_file test/expected/performance/401_import_benchmark_queries.perf
\pset tuples_only on
\pset footer off
\o :queries_file
SELECT '# Slow Query Analysis from pg_stat_monitor';
SELECT '# Queries sorted by total execution time';
SELECT '#';
SELECT '';
\pset tuples_only off
-- Output top slow queries if pg_stat_monitor is available
DO $$
BEGIN
    IF EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_monitor') THEN
        -- Create temp table with query stats
        CREATE TEMP TABLE temp_query_stats AS
        SELECT
            queryid::text,
            calls,
            ROUND(total_exec_time::numeric, 2) AS total_exec_time_ms,
            ROUND((total_exec_time / NULLIF(calls, 0))::numeric, 2) AS avg_exec_time_ms,
            rows,
            ROUND((shared_blks_hit + shared_blks_read)::numeric / NULLIF(calls, 0), 0) AS avg_blks_per_call,
            LEFT(regexp_replace(query, E'[\\n\\r]+', ' ', 'g'), 200) AS query_preview
        FROM pg_stat_monitor
        WHERE total_exec_time > 10  -- Only queries taking > 10ms total
        ORDER BY total_exec_time DESC
        LIMIT 30;
    ELSE
        CREATE TEMP TABLE temp_query_stats AS
        SELECT 
            'N/A'::text AS queryid,
            0::bigint AS calls,
            0::numeric AS total_exec_time_ms,
            0::numeric AS avg_exec_time_ms,
            0::bigint AS rows,
            0::numeric AS avg_blks_per_call,
            'pg_stat_monitor extension not available'::text AS query_preview
        WHERE false;
    END IF;
END;
$$;
SELECT '# Top queries by total execution time:' as "header";
SELECT * FROM temp_query_stats ORDER BY total_exec_time_ms DESC;
\o
\pset footer on
\pset tuples_only off
COMMIT;
-- ============================================================================
-- CLEANUP
-- ============================================================================
\echo 'Resuming background worker'
Resuming background worker
SELECT worker.resume();
 resume 
--------
 
(1 row)

\i test/cleanup_unless_persist_is_specified.sql
---------------------------------------------------------------------------
-- Support development loading of the data without cleanup using
--   ./devops/manage-statbus.sh psql --variable=PERSIST=true < test/sql/400_import_jobs_for_norway_history.sql
--
-- This script is used for tests that commit during execution (e.g., to allow
-- worker.process_tasks() to commit between tasks for better performance).
-- Unlike rollback_unless_persist_is_specified.sql, this performs explicit
-- cleanup since ROLLBACK cannot undo committed transactions.
--
-- NOTE: For 4xx tests running in isolated databases (via test-isolated command),
-- cleanup is unnecessary since the entire database is dropped after the test.
-- We skip the slow reset() call when running in an isolated test database.
-- Ref. https://stackoverflow.com/a/32597876/1023558
\set PERSIST :PERSIST
-- now PERSIST is set to the string ':PERSIST' if was not already set.
-- Checking it using a CASE statement:
SELECT CASE
  WHEN :'PERSIST'= ':PERSIST'
  THEN 'false'
  ELSE :'PERSIST'
END::BOOL AS "PERSIST" \gset
-- < \gset call at end of the query to set variable.
-- Check if we're in an isolated test database (name starts with 'test_')
SELECT current_database() LIKE 'test_%' AS "IS_ISOLATED_TEST" \gset
\if :PERSIST
\echo 'PERSIST=true: Keeping test data for inspection'
\elif :IS_ISOLATED_TEST
\echo 'Isolated test database - skipping cleanup (database will be dropped)'
Isolated test database - skipping cleanup (database will be dropped)
\else
\echo 'Cleaning up test data (use PERSIST=true to keep)'
-- Clean up worker tasks first (may reference import jobs)
DELETE FROM worker.tasks WHERE state IN ('completed', 'failed');
-- Use the reset function to clean up all test data
-- 'data' scope removes: import_jobs, units, activities, locations, etc.
-- but preserves configuration (regions, settings, activity categories)
SELECT public.reset(true, 'data');
\endif
