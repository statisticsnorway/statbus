BEGIN;
\i test/setup.sql
-- While the datestyle is set for the database, the pg_regress tool sets the MDY format
-- to ensure consistent date formatting, so we must manually override this
SET datestyle TO 'ISO, DMY';
\if :{?DEBUG}
SET client_min_messages TO debug1;
\else
SET client_min_messages TO NOTICE;
\endif
-- Create temporary function to execute queries as system user
CREATE OR REPLACE FUNCTION test.sudo_exec(
    sql text,
    OUT results jsonb
) RETURNS jsonb
SECURITY DEFINER LANGUAGE plpgsql AS $sudo_exec$
DECLARE
    result_rows jsonb;
BEGIN
    -- Check if the SQL starts with common DDL keywords
    IF sql ~* '^\s*(CREATE|DROP|ALTER|TRUNCATE|GRANT|REVOKE|ANALYZE)' THEN
        -- For DDL statements, execute directly
        EXECUTE sql;
        results := '[]'::jsonb;
    ELSE
        -- For DML/queries, wrap in a SELECT to capture results
        EXECUTE format('
            SELECT COALESCE(
                jsonb_agg(row_to_json(t)),
                ''[]''::jsonb
            )
            FROM (%s) t',
            sql
        ) INTO result_rows;
        results := result_rows;
    END IF;
END;
$sudo_exec$;
-- Grant execute to public since this is for testing
GRANT EXECUTE ON FUNCTION test.sudo_exec(text) TO PUBLIC;
\echo Add users for testing purposes
Add users for testing purposes
SELECT * FROM public.user_create('test.admin@statbus.org', 'admin_user'::statbus_role, 'Admin#123!');
         email          |  password  
------------------------+------------
 test.admin@statbus.org | Admin#123!
(1 row)

SELECT * FROM public.user_create('test.regular@statbus.org', 'regular_user'::statbus_role, 'Regular#123!');
          email           |   password   
--------------------------+--------------
 test.regular@statbus.org | Regular#123!
(1 row)

SELECT * FROM public.user_create('test.restricted@statbus.org', 'restricted_user'::statbus_role, 'Restricted#123!');
            email            |    password     
-----------------------------+-----------------
 test.restricted@statbus.org | Restricted#123!
(1 row)

\echo '----------------------------------------------------------------------------'
----------------------------------------------------------------------------
\echo 'Test: admin.batch_insert_or_replace_generic_valid_time_table'
Test: admin.batch_insert_or_replace_generic_valid_time_table
\echo '----------------------------------------------------------------------------'
----------------------------------------------------------------------------
SET client_min_messages TO NOTICE;
-- Setup: Create necessary schema and tables
CREATE SCHEMA IF NOT EXISTS batch_test; -- Use dedicated schema
CREATE SEQUENCE IF NOT EXISTS batch_test.batch_upsert_target_id_seq;
NOTICE:  Granted USAGE on new sequence batch_test.batch_upsert_target_id_seq to authenticated
-- Target table for the upsert operation
CREATE TABLE batch_test.batch_upsert_target (
    id INT NOT NULL DEFAULT nextval('batch_test.batch_upsert_target_id_seq'),
    valid_after DATE NOT NULL, -- (exclusive start)
    valid_to DATE NOT NULL,    -- (inclusive end)
    value_a TEXT,
    value_b INT,
    edit_comment TEXT, -- Ephemeral column
    PRIMARY KEY (id, valid_after) -- PK uses valid_after
);
-- Source table containing data to be upserted
CREATE TABLE batch_test.batch_upsert_source (
    row_id BIGSERIAL PRIMARY KEY, -- Source row identifier
    founding_row_id BIGINT,       -- Identifies the original source row that founded an entity
    target_id INT, -- ID in the target table (can be null for lookup)
    valid_after DATE NOT NULL, -- (exclusive start)
    valid_to DATE,             -- (inclusive end)
    value_a TEXT,
    value_b INT,
    edit_comment TEXT
);
-- Parameters for the batch upsert function
\set target_schema 'batch_test'
\set target_table 'batch_upsert_target'
\set source_schema 'batch_test'
\set source_table 'batch_upsert_source'
-- \set source_row_id_col 'row_id' -- Removed, function expects 'row_id'
-- Define variables without outer SQL quotes
\set unique_cols '[ "value_a" ]'
-- \set temporal_cols '{valid_after, valid_to}' -- Removed, function expects 'valid_after', 'valid_to'
\set ephemeral_cols '{edit_comment}'
\set id_col 'id'
-- Function to display target table contents easily
CREATE OR REPLACE FUNCTION batch_test.show_target_table(p_filter_id INT DEFAULT NULL)
RETURNS TABLE (id INT, valid_after DATE, valid_to DATE, value_a TEXT, value_b INT, edit_comment TEXT) AS $$
BEGIN
    IF p_filter_id IS NULL THEN
        RETURN QUERY SELECT tgt.id, tgt.valid_after, tgt.valid_to, tgt.value_a, tgt.value_b, tgt.edit_comment 
                     FROM batch_test.batch_upsert_target tgt ORDER BY tgt.id, tgt.valid_after;
    ELSE
        RETURN QUERY SELECT tgt.id, tgt.valid_after, tgt.valid_to, tgt.value_a, tgt.value_b, tgt.edit_comment 
                     FROM batch_test.batch_upsert_target tgt WHERE tgt.id = p_filter_id ORDER BY tgt.id, tgt.valid_after;
    END IF;
END;
$$ LANGUAGE plpgsql;
-- 1. Initial Insert
\echo 'Scenario 1: Initial Insert'
Scenario 1: Initial Insert
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, NULL, '2023-12-31', '2024-12-31', 'A', 10, 'Initial A'); -- row_id will be 1, founding_row_id = 1
-- Use SQL quotes around the variable, then cast
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(); -- Show all, expect one row with new ID
 id | valid_after |  valid_to  | value_a | value_b | edit_comment 
----+-------------+------------+---------+---------+--------------
  1 | 2023-12-31  | 2024-12-31 | A       |      10 | Initial A
(1 row)

TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE;
DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 2. Adjacent Equivalent Merge
\echo 'Scenario 2: Adjacent Equivalent Merge'
Scenario 2: Adjacent Equivalent Merge
-- SET client_min_messages TO DEBUG1; -- Scenario 2 is now passing
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, '2023-12-31', '2024-06-30', 'A', 10, 'First half');
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2024-06-30', '2024-12-31', 'A', 10, 'Second half'); -- target_id known, founding_row_id is NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1); -- Should be one merged row for ID 1
 id | valid_after |  valid_to  | value_a | value_b | edit_comment 
----+-------------+------------+---------+---------+--------------
  1 | 2023-12-31  | 2024-12-31 | A       |      10 | Second half
(1 row)

-- SET client_min_messages TO NOTICE; -- Already NOTICE by default
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 3. Adjacent Different (No Merge)
\echo 'Scenario 3: Adjacent Different (No Merge)'
Scenario 3: Adjacent Different (No Merge)
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, '2023-12-31', '2024-06-30', 'A', 10, 'First half');
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2024-06-30', '2024-12-31', 'B', 20, 'Second half different'); -- target_id known, founding_row_id is NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1); -- Should be two separate rows for ID 1
 id | valid_after |  valid_to  | value_a | value_b |     edit_comment      
----+-------------+------------+---------+---------+-----------------------
  1 | 2023-12-31  | 2024-06-30 | A       |      10 | First half
  1 | 2024-06-30  | 2024-12-31 | B       |      20 | Second half different
(2 rows)

TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 4. Overlap Start Equivalent
\echo 'Scenario 4: Overlap Start Equivalent'
Scenario 4: Overlap Start Equivalent
-- SET client_min_messages TO DEBUG1;
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, '2024-02-29', '2024-12-31', 'A', 10, 'Existing March-Dec'); -- (Feb 29, Dec 31]
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2023-12-31', '2024-05-31', 'A', 10, 'New Jan-May'); -- target_id known, founding_row_id is NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1); -- Should be one row Jan-Dec for ID 1
 id | valid_after |  valid_to  | value_a | value_b | edit_comment 
----+-------------+------------+---------+---------+--------------
  1 | 2023-12-31  | 2024-12-31 | A       |      10 | New Jan-May
(1 row)

-- SET client_min_messages TO NOTICE;
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 5. Overlap Start Different
\echo 'Scenario 5: Overlap Start Different'
Scenario 5: Overlap Start Different
-- SET client_min_messages TO DEBUG1; -- Scenario 5 is now passing
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, '2024-02-29', '2024-12-31', 'A', 10, 'Existing March-Dec'); -- (Feb 29, Dec 31]
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2023-12-31', '2024-05-31', 'B', 20, 'New Jan-May Different'); -- target_id known, founding_row_id is NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1); -- Should be New Jan-May (B), Existing June-Dec (A) for ID 1
 id | valid_after |  valid_to  | value_a | value_b |     edit_comment      
----+-------------+------------+---------+---------+-----------------------
  1 | 2023-12-31  | 2024-05-31 | B       |      20 | New Jan-May Different
  1 | 2024-05-31  | 2024-12-31 | A       |      10 | Existing March-Dec
(2 rows)

-- SET client_min_messages TO NOTICE; -- Already NOTICE by default
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 6. Overlap End Equivalent
\echo 'Scenario 6: Overlap End Equivalent'
Scenario 6: Overlap End Equivalent
-- SET client_min_messages TO DEBUG1;
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, '2023-12-31', '2024-09-30', 'A', 10, 'Existing Jan-Sep'); -- (Dec 31, Sep 30]
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2024-06-30', '2024-12-31', 'A', 10, 'New Jul-Dec'); -- target_id known, founding_row_id is NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1); -- Should be one row Jan-Dec for ID 1
 id | valid_after |  valid_to  | value_a | value_b | edit_comment 
----+-------------+------------+---------+---------+--------------
  1 | 2023-12-31  | 2024-12-31 | A       |      10 | New Jul-Dec
(1 row)

-- SET client_min_messages TO NOTICE;
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 7. Overlap End Different
\echo 'Scenario 7: Overlap End Different'
Scenario 7: Overlap End Different
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, '2023-12-31', '2024-09-30', 'A', 10, 'Existing Jan-Sep'); -- (Dec 31, Sep 30]
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2024-06-30', '2024-12-31', 'B', 20, 'New Jul-Dec Different'); -- target_id known, founding_row_id is NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1); -- Should be Existing Jan-Jun (A), New Jul-Dec (B) for ID 1
 id | valid_after |  valid_to  | value_a | value_b |     edit_comment      
----+-------------+------------+---------+---------+-----------------------
  1 | 2023-12-31  | 2024-06-30 | A       |      10 | Existing Jan-Sep
  1 | 2024-06-30  | 2024-12-31 | B       |      20 | New Jul-Dec Different
(2 rows)

TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 8. Inside Equivalent
\echo 'Scenario 8: Inside Equivalent'
Scenario 8: Inside Equivalent
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, '2023-12-31', '2024-12-31', 'A', 10, 'Existing Jan-Dec'); -- (Dec 31, Dec 31]
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2024-03-31', '2024-08-31', 'A', 10, 'New Apr-Aug'); -- target_id known, founding_row_id is NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1); -- Should be one row Jan-Dec for ID 1
 id | valid_after |  valid_to  | value_a | value_b | edit_comment 
----+-------------+------------+---------+---------+--------------
  1 | 2023-12-31  | 2024-12-31 | A       |      10 | New Apr-Aug
(1 row)

TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 9. Inside Different (Split)
\echo 'Scenario 9: Inside Different (Split)'
Scenario 9: Inside Different (Split)
-- SET client_min_messages TO DEBUG1; -- Scenario 9 is now passing
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, '2023-12-31', '2024-12-31', 'A', 10, 'Existing Jan-Dec'); -- (Dec 31, Dec 31]
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2024-03-31', '2024-08-31', 'B', 20, 'New Apr-Aug Different'); -- target_id known, founding_row_id is NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1); -- Should be Existing Jan-Mar (A), New Apr-Aug (B), Existing Sep-Dec (A) for ID 1
 id | valid_after |  valid_to  | value_a | value_b |     edit_comment      
----+-------------+------------+---------+---------+-----------------------
  1 | 2023-12-31  | 2024-03-31 | A       |      10 | Existing Jan-Dec
  1 | 2024-03-31  | 2024-08-31 | B       |      20 | New Apr-Aug Different
  1 | 2024-08-31  | 2024-12-31 | A       |      10 | Existing Jan-Dec
(3 rows)

-- SET client_min_messages TO NOTICE; -- Already NOTICE by default
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 10. Contains Existing
\echo 'Scenario 10: Contains Existing'
Scenario 10: Contains Existing
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, '2024-03-31', '2024-08-31', 'A', 10, 'Existing Apr-Aug'); -- (Mar 31, Aug 31]
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2023-12-31', '2024-12-31', 'B', 20, 'New Jan-Dec Different'); -- target_id known, founding_row_id is NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1); -- Should be one row Jan-Dec with value B for ID 1
 id | valid_after |  valid_to  | value_a | value_b |     edit_comment      
----+-------------+------------+---------+---------+-----------------------
  1 | 2023-12-31  | 2024-12-31 | B       |      20 | New Jan-Dec Different
(1 row)

TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 11. Batch Processing (Multiple IDs, Multiple Scenarios)
\echo 'Scenario 11: Batch Processing (Multiple IDs and Scenarios)'
Scenario 11: Batch Processing (Multiple IDs and Scenarios)
-- ID 1 (will be new, e.g. seq val 1): Initial Insert
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, NULL, '2023-12-31', '2024-12-31', 'ID1', 11, 'ID1 Initial'); -- row_id=1, founding_row_id=1 (New entity)
-- ID 2: Existing data
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(2, '2023-12-31', '2024-12-31', 'ID2-Old', 22, 'ID2 Existing');
-- ID 2: Source data to split existing ID2
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 2, '2024-04-30', '2024-08-31', 'ID2-New', 23, 'ID2 Split'); -- row_id=2, target_id known, founding_row_id=NULL
-- ID 3: Existing data
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(3, '2023-12-31', '2024-06-30', 'ID3', 33, 'ID3 First Half');
-- ID 3: Source data adjacent equivalent to merge
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 3, '2024-06-30', '2024-12-31', 'ID3', 33, 'ID3 Second Half Merge'); -- row_id=3, target_id known, founding_row_id=NULL
-- ID 4: Source data with error (null valid_to)
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 4, '2023-12-31', NULL, 'ID4-Error', 44, 'ID4 Error'); -- row_id=4, target_id known, founding_row_id=NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
) ORDER BY source_row_id;
WARNING:  [batch_replace] Error processing source_row_id 4 ({"id": 4, "row_id": 4, "value_a": "ID4-Error", "value_b": 44, "valid_to": null, "target_id": 4, "valid_after": "2023-12-31", "edit_comment": "ID4 Error", "founding_row_id": null}): Temporal columns ('valid_after', 'valid_to') cannot be null. Error in source row with 'row_id' = 4: {"id": 4, "row_id": 4, "value_a": "ID4-Error", "value_b": 44, "valid_to": null, "target_id": 4, "valid_after": "2023-12-31", "edit_comment": "ID4 Error", "founding_row_id": null}. Context: PL/pgSQL function import.batch_insert_or_replace_generic_valid_time_table(text,text,text,text,jsonb,text[],text,text[]) line 155 at RAISE
 source_row_id | upserted_record_id | status  |                                                                                                                                     error_message                                                                                                                                      
---------------+--------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
             1 |                  1 | SUCCESS | 
             2 |                  2 | SUCCESS | 
             3 |                  3 | SUCCESS | 
             4 |                    | ERROR   | Temporal columns ('valid_after', 'valid_to') cannot be null. Error in source row with 'row_id' = 4: {"id": 4, "row_id": 4, "value_a": "ID4-Error", "value_b": 44, "valid_to": null, "target_id": 4, "valid_after": "2023-12-31", "edit_comment": "ID4 Error", "founding_row_id": null}
(4 rows)

\echo 'Target table after batch:'
Target table after batch:
SELECT * FROM batch_test.show_target_table();
 id | valid_after |  valid_to  | value_a | value_b |     edit_comment      
----+-------------+------------+---------+---------+-----------------------
  1 | 2023-12-31  | 2024-12-31 | ID1     |      11 | ID1 Initial
  2 | 2023-12-31  | 2024-04-30 | ID2-Old |      22 | ID2 Existing
  2 | 2024-04-30  | 2024-08-31 | ID2-New |      23 | ID2 Split
  2 | 2024-08-31  | 2024-12-31 | ID2-Old |      22 | ID2 Existing
  3 | 2023-12-31  | 2024-12-31 | ID3     |      33 | ID3 Second Half Merge
(5 rows)

-- Expected:
-- ID for ID1 (e.g. 1): One row Jan-Dec, value ID1
-- ID 2: Three rows: Jan-Apr (ID2-Old), May-Aug (ID2-New), Sep-Dec (ID2-Old)
-- ID 3: One row Jan-Dec, value ID3
-- ID 4: No rows in target table
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 12. ID Lookup using unique_columns
\echo 'Scenario 12: ID Lookup'
Scenario 12: ID Lookup
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(5, '2023-12-31', '2024-12-31', 'LookupMe', 50, 'Existing Lookup');
-- Source row has target_id = NULL, but value_a matches existing row
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, NULL, '2024-05-31', '2024-09-30', 'LookupMe', 55, 'Update via Lookup'); -- target_id NULL for lookup, founding_row_id NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  5 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(5); -- Should have split the original row for ID 5
 id | valid_after |  valid_to  | value_a  | value_b |   edit_comment    
----+-------------+------------+----------+---------+-------------------
  5 | 2023-12-31  | 2024-05-31 | LookupMe |      50 | Existing Lookup
  5 | 2024-05-31  | 2024-09-30 | LookupMe |      55 | Update via Lookup
  5 | 2024-09-30  | 2024-12-31 | LookupMe |      50 | Existing Lookup
(3 rows)

TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 13. Identical Period, Different Data, Full Replacement
\echo 'Scenario 13: Identical Period, Different Data, Full Replacement'
Scenario 13: Identical Period, Different Data, Full Replacement
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, '2023-12-31', '2024-12-31', 'KeyForID1', 100, 'Original Version');
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2023-12-31', '2024-12-31', 'KeyForID1', 200, 'Updated Version'); -- target_id known, founding_row_id is NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => '[]'::JSONB, -- p_unique_columns is empty as target_id is provided
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1); -- Should be one row for ID 1, with value_b = 200
 id | valid_after |  valid_to  |  value_a  | value_b |  edit_comment   
----+-------------+------------+-----------+---------+-----------------
  1 | 2023-12-31  | 2024-12-31 | KeyForID1 |     200 | Updated Version
(1 row)

TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 14. Equals Relation, Equivalent Data (No Change Expected)
\echo 'Scenario 14: Equals Relation, Equivalent Data'
Scenario 14: Equals Relation, Equivalent Data
-- SET client_min_messages TO DEBUG1; -- Scenario 14 is now passing
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, '2023-12-31', '2024-12-31', 'Equivalent', 100, 'Original Comment');
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2023-12-31', '2024-12-31', 'Equivalent', 100, 'Source Comment, Should Update Ephemeral'); -- target_id known, founding_row_id is NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => '[]'::JSONB, -- p_unique_columns is empty as target_id is provided
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1); -- Expected: One row for ID 1, edit_comment = 'Source Comment, Should Update Ephemeral'
 id | valid_after |  valid_to  |  value_a   | value_b |              edit_comment               
----+-------------+------------+------------+---------+-----------------------------------------
  1 | 2023-12-31  | 2024-12-31 | Equivalent |     100 | Source Comment, Should Update Ephemeral
(1 row)

-- SET client_min_messages TO NOTICE; -- Reverted as scenario is passing
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 15. Precedes Relation (Non-Overlapping, New record should be added)
\echo 'Scenario 15: Precedes Relation'
Scenario 15: Precedes Relation
INSERT INTO batch_test.batch_upsert_target (id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, '2024-06-30', '2024-12-31', 'Later', 200, 'Later Record'); -- Existing: (June 30, Dec 31]
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2023-12-31', '2024-03-31', 'Earlier', 100, 'Earlier Record'); -- target_id known, founding_row_id is NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => '[]'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1); -- Expected: Two rows for ID 1, one for earlier, one for later period
 id | valid_after |  valid_to  | value_a | value_b |  edit_comment  
----+-------------+------------+---------+---------+----------------
  1 | 2023-12-31  | 2024-03-31 | Earlier |     100 | Earlier Record
  1 | 2024-06-30  | 2024-12-31 | Later   |     200 | Later Record
(2 rows)

TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- 16. A-A-B-A Sequence with yearly 'infinity' inputs (4 inputs -> 3 outputs)
\echo 'Scenario 16: A-A-B-A Sequence with yearly ''infinity'' inputs (4 inputs -> 3 outputs)'
Scenario 16: A-A-B-A Sequence with yearly 'infinity' inputs (4 inputs -> 3 outputs)
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target; ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- Step 1: Insert A1 (CoreA, val_b=10) for 2021 (valid_to=infinity)
\echo 'Step 16.1: Insert A1 (CoreA, val_b=10) for 2021 (valid_to=infinity)'
Step 16.1: Insert A1 (CoreA, val_b=10) for 2021 (valid_to=infinity)
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(1, NULL, '2020-12-31', 'infinity', 'CoreA', 10, 'CommentA1_2021'); -- row_id=1, founding_row_id=1
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table();
 id | valid_after | valid_to | value_a | value_b |  edit_comment  
----+-------------+----------+---------+---------+----------------
  1 | 2020-12-31  | infinity | CoreA   |      10 | CommentA1_2021
(1 row)

-- Expected: 1 row for ID 1: (CoreA,10) valid_after='2020-12-31', valid_to='infinity', comment='CommentA1_2021'
-- Step 2: Insert A2 (CoreA, val_b=10 - same core, same ephemeral for simplicity of merge test) for 2022 (valid_to=infinity)
\echo 'Step 16.2: Insert A2 (CoreA, val_b=10) for 2022 (valid_to=infinity) - meets A1, equivalent core'
Step 16.2: Insert A2 (CoreA, val_b=10) for 2022 (valid_to=infinity) - meets A1, equivalent core
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE;
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2021-12-31', 'infinity', 'CoreA', 10, 'CommentA2_2022'); -- target_id known, founding_row_id=NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => '[]'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1);
 id | valid_after | valid_to | value_a | value_b |  edit_comment  
----+-------------+----------+---------+---------+----------------
  1 | 2020-12-31  | infinity | CoreA   |      10 | CommentA2_2022
(1 row)

-- Expected: 1 row for ID 1: (CoreA,10) valid_after='2020-12-31', valid_to='infinity', comment='CommentA2_2022' (original A1 is closed at 2021-12-31, A2 merges and extends)
-- Step 3: Insert B1 (CoreB, val_b=20) for 2023 (valid_to=infinity) - different core
\echo 'Step 16.3: Insert B1 (CoreB, val_b=20) for 2023 (valid_to=infinity) - meets A1A2, different core'
Step 16.3: Insert B1 (CoreB, val_b=20) for 2023 (valid_to=infinity) - meets A1A2, different core
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE;
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2022-12-31', 'infinity', 'CoreB', 20, 'CommentB1_2023'); -- target_id known, founding_row_id=NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => '[]'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1);
 id | valid_after |  valid_to  | value_a | value_b |  edit_comment  
----+-------------+------------+---------+---------+----------------
  1 | 2020-12-31  | 2022-12-31 | CoreA   |      10 | CommentA2_2022
  1 | 2022-12-31  | infinity   | CoreB   |      20 | CommentB1_2023
(2 rows)

-- Expected: 2 rows for ID 1:
-- 1: (CoreA,10) valid_after='2020-12-31', valid_to='2022-12-31', comment='CommentA2_2022'
-- 2: (CoreB,20) valid_after='2022-12-31', valid_to='infinity', comment='CommentB1_2023'
-- Step 4: Insert A3 (CoreA, val_b=10) for 2024 (valid_to=infinity) - different from B1, same core as A1A2
\echo 'Step 16.4: Insert A3 (CoreA, val_b=10) for 2024 (valid_to=infinity) - meets B1, different core from B1'
Step 16.4: Insert A3 (CoreA, val_b=10) for 2024 (valid_to=infinity) - meets B1, different core from B1
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE;
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2023-12-31', 'infinity', 'CoreA', 10, 'CommentA3_2024'); -- target_id known, founding_row_id=NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => '[]'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1);
 id | valid_after |  valid_to  | value_a | value_b |  edit_comment  
----+-------------+------------+---------+---------+----------------
  1 | 2020-12-31  | 2022-12-31 | CoreA   |      10 | CommentA2_2022
  1 | 2022-12-31  | 2023-12-31 | CoreB   |      20 | CommentB1_2023
  1 | 2023-12-31  | infinity   | CoreA   |      10 | CommentA3_2024
(3 rows)

-- Expected: 3 rows for ID 1:
-- 1: (CoreA,10) valid_after='2020-12-31', valid_to='2022-12-31', comment='CommentA2_2022'
-- 2: (CoreB,20) valid_after='2022-12-31', valid_to='2023-12-31', comment='CommentB1_2023'
-- 3: (CoreA,10) valid_after='2023-12-31', valid_to='infinity', comment='CommentA3_2024'
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- New Scenario: Multiple Edits to Same New Entity in One Batch
\echo 'Scenario N1: Multiple Edits to Same New Entity in One Batch (founding_row_id cache test)'
Scenario N1: Multiple Edits to Same New Entity in One Batch (founding_row_id cache test)
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE;
DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- Both source rows have target_id=NULL but share founding_row_id=101.
-- The first should create a new entity (e.g. ID=1).
-- The second should use the cached ID=1 due to matching founding_row_id.
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(101, NULL, '2023-01-01', '2023-06-30', 'MultiEditNew', 10, 'First part new'), -- row_id will be 1
(101, NULL, '2023-07-01', '2023-12-31', 'MultiEditNew', 20, 'Second part new, value_b changed'); -- row_id will be 2
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
) ORDER BY source_row_id;
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
             2 |                  1 | SUCCESS | 
(2 rows)

SELECT * FROM batch_test.show_target_table();
 id | valid_after |  valid_to  |   value_a    | value_b |           edit_comment           
----+-------------+------------+--------------+---------+----------------------------------
  1 | 2023-01-01  | 2023-06-30 | MultiEditNew |      10 | First part new
  1 | 2023-07-01  | 2023-12-31 | MultiEditNew |      20 | Second part new, value_b changed
(2 rows)

-- Expected: A single new ID (e.g., 1) should be created for 'MultiEditNew'.
-- Two rows for this ID due to different value_b in different periods.
-- ID (e.g. 1): (MultiEditNew, 10) valid_after='2023-01-01', valid_to='2023-06-30'
-- ID (e.g. 1): (MultiEditNew, 20) valid_after='2023-07-01', valid_to='2023-12-31'
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- New Scenario N2: Rolling Infinity Updates - Equivalent Core Data
\echo 'Scenario N2: Rolling Infinity Updates - Equivalent Core Data (batch_insert_or_replace)'
Scenario N2: Rolling Infinity Updates - Equivalent Core Data (batch_insert_or_replace)
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE;
DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- Step N2.1: Insert initial record with valid_to = infinity
\echo 'Step N2.1: Insert Year 1 data (valid_to=infinity)'
Step N2.1: Insert Year 1 data (valid_to=infinity)
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(201, NULL, '2022-12-31', 'infinity', 'RollingReplace', 100, 'Year 1 Comment'); -- New entity, founding_row_id 201
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => :'unique_cols'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table();
 id | valid_after | valid_to |    value_a     | value_b |  edit_comment  
----+-------------+----------+----------------+---------+----------------
  1 | 2022-12-31  | infinity | RollingReplace |     100 | Year 1 Comment
(1 row)

-- Expected: 1 row for new ID (e.g. 1), (RollingReplace, 100), va='2022-12-31', vt='infinity', comment='Year 1 Comment'
-- Step N2.2: Update with a new record that meets the previous one, same core data, new ephemeral data
\echo 'Step N2.2: Insert Year 2 data (valid_to=infinity, same core, new ephemeral)'
Step N2.2: Insert Year 2 data (valid_to=infinity, same core, new ephemeral)
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE;
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2023-12-31', 'infinity', 'RollingReplace', 100, 'Year 2 Comment'); -- target_id known, founding_row_id=NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => '[]'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1);
 id | valid_after | valid_to |    value_a     | value_b |  edit_comment  
----+-------------+----------+----------------+---------+----------------
  1 | 2022-12-31  | infinity | RollingReplace |     100 | Year 2 Comment
(1 row)

-- Expected: 1 row for ID 1. Original period ('2022-12-31', 'infinity') is closed at '2023-12-31'.
-- New period ('2023-12-31', 'infinity') is inserted. Since core data is same, it effectively merges.
-- Result: (RollingReplace, 100), va='2022-12-31', vt='infinity', comment='Year 2 Comment' (ephemeral updated)
-- Step N2.3: Update again with the same period, same core data, different ephemeral data
\echo 'Step N2.3: Insert Year 2 data again (valid_to=infinity, same core, different ephemeral again)'
Step N2.3: Insert Year 2 data again (valid_to=infinity, same core, different ephemeral again)
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE;
INSERT INTO batch_test.batch_upsert_source (founding_row_id, target_id, valid_after, valid_to, value_a, value_b, edit_comment) VALUES
(NULL, 1, '2023-12-31', 'infinity', 'RollingReplace', 100, 'Year 2 Comment Rev2'); -- target_id known, founding_row_id=NULL
SELECT * FROM import.batch_insert_or_replace_generic_valid_time_table(
    p_target_schema_name           => :'target_schema',
    p_target_table_name            => :'target_table',
    p_source_schema_name           => :'source_schema',
    p_source_table_name            => :'source_table',
    p_unique_columns               => '[]'::JSONB,
    p_ephemeral_columns            => :'ephemeral_cols'::TEXT[],
    p_id_column_name               => :'id_col',
    p_generated_columns_override   => NULL
);
 source_row_id | upserted_record_id | status  | error_message 
---------------+--------------------+---------+---------------
             1 |                  1 | SUCCESS | 
(1 row)

SELECT * FROM batch_test.show_target_table(1);
 id | valid_after | valid_to |    value_a     | value_b |    edit_comment     
----+-------------+----------+----------------+---------+---------------------
  1 | 2022-12-31  | infinity | RollingReplace |     100 | Year 2 Comment Rev2
(1 row)

-- Expected: 1 row for ID 1. The existing period ('2023-12-31', 'infinity') is replaced because it's an exact match on period.
-- Result: (RollingReplace, 100), va='2022-12-31', vt='infinity', comment='Year 2 Comment Rev2' (ephemeral updated again)
TRUNCATE batch_test.batch_upsert_source RESTART IDENTITY CASCADE; DELETE FROM batch_test.batch_upsert_target;
ALTER SEQUENCE batch_test.batch_upsert_target_id_seq RESTART WITH 1;
-- Cleanup
DROP FUNCTION batch_test.show_target_table(INT);
DROP TABLE batch_test.batch_upsert_source;
DROP TABLE batch_test.batch_upsert_target;
DROP SEQUENCE batch_test.batch_upsert_target_id_seq;
DROP SCHEMA batch_test CASCADE; -- Use CASCADE to drop schema and its contents
SET client_min_messages TO NOTICE; -- Keep NOTICE for the final ROLLBACK message
ROLLBACK; -- Rollback changes after test
